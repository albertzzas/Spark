Spark 2.0新特性介绍



Spark Core&Spark SQL API
* dataframe与dataset统一， dataframe只是dataset[Row]的类型别名
* SparkSession：统一SQLContext和HiveContext，新的上下文入口
* 为SparkSession开发的一种新的流式调用的configuration api
* accumulator功能增强：便捷api、 web ui支持、性能更高
* dataset的增强聚合api


Spark Core&Spark SQL
SQL
* 支持sql 2003标准
* 支持ansi-sql和hive ql的sql parser
* 支持ddl命令
* 支持子查询： in/not in、 exists/not exists


Spark Core&Spark
SQL
new feature
* 支持csv文件
* 支持缓存和程序运行的堆外内存管理
* 支持hive风格的bucket表
* 支持近似概要统计，包括近似分位数、布隆过滤器、最小
略图


Spark Core&Spark
SQL
性能
* 通过whole-stage code generation技术将spark sql和dataset的性能提升2~10倍
* 通过vectorization技术提升parquet文件的扫描吞吐量
* 提升orc文件的读写性能
* 提升catalyst查询优化器的性能
* 通过native实现方式提升窗口函数的性能
* 对某些数据源进行自动文件合并


Spark MLlib
* spark mllib未来将主要基于dataset api来实现，基于rdd的api转为维护阶段
* 基于dataframe的api，支持持久化保存和加载模型和pipeline
* 基于dataframe的api，支持更多算法，包括二分kmeans、高斯混合、
maxabsscaler等
* spark R支持mllib算法，包括线性回归、朴素贝叶斯、 kmeans、多元回归等
* pyspark支持更多mllib算法，包括LDA、高斯混合、泛化线性回顾等
* 基于dataframe的api，向量和矩阵使用性能更高的序列化机制


Spark Streaming
* 发布测试版的structured streaming
* 基于spark sql和catalyst引擎构建
* 支持使用dataframe风格的api进行流式计算操作
* catalyst引擎能够对执行计划进行优化
* 基于dstream的api支持kafka 0.10版本


依赖管理、打包和操作
* 不再需要在生产环境部署时打包fat jar，可以使用provided风格
* 完全移除了对akka的依赖
* mesos粗粒度模式下，支持启动多个executor
* 支持kryo 3.0版本
* 使用scala 2.11替代了scala 2.10


移除的功能
* bagel模块
* 对hadoop 2.1以及之前版本的支持
* 闭包序列化配置的支持
* HTTPBroadcast支持
* 基于TTL模式的元数据清理支持
* 半私有的org.apache.spark.Logging的使用支持
* SparkContext.metricsSystem API
* 与tachyon的面向block的整合支持
* spark 1.x中标识为过期的所有api
* python dataframe中返回rdd的方法
* 使用很少的streaming数据源支持： twitter、 akka、 MQTT、 ZeroMQ
* hash-based shuffle manager
* standalone master的历史数据支持功能
* dataframe不再是一个类，而是dataset[Row]的类型别名


变化的机制
* 要求基于scala 2.11版本进行开发，而不是scala 2.10版本
* SQL中的浮点类型，使用decimal类型来表示，而不是double类型
* kryo版本升级到了3.0
* java的flatMap和mapPartitions方法，从iterable类型转变为iterator类型
* java的countByKey返回<K,Long>类型，而不是<K,Object>类型
* 写parquet文件时， summary文件默认不会写了，需要开启参数来启用
* spark mllib中，基于dataframe的api完全依赖于自己，不再依赖mllib包


过期的API
* mesos的细粒度模式
* java 7支持标识为过期，可能2.x未来版本会移除支持
* python 2.6的支持


Spark 2.0-易用性：标准化SQL支持以及更合理的API



标准化SQL支持以及更合理
的API
Spark最引以为豪的几个特点就是简单、直观、表达性好。 Spark 2.0为了继续加强这几个特点，
做了两件事情： 1、提供标准化的SQL支持； 2、统一了Dataframe和Dataset两套API。
在标准化SQL支持方面，引入了新的ANSI-SQL解析器，提供标准化SQL的解析功能，而且还提供
了子查询的支持。 Spark现在可以运行完整的99个TPC-DS查询，这就要求Spark包含大多数SQL
2003标准的特性。这么做的好处在于， SQL一直是大数据应用领域的一个最广泛接受的标准，
比如说Hadoop，做大数据的企业90%的时间都在用Hive，写SQL做各种大数据的统计和分析。
因此Spark SQL提升对SQL的支持，可以大幅度减少用户将应用从其他技术（比如Oracle、 Hive等）
迁移过来的成本。


统一Dataframe和Dataset
API
从Spark 2.0开始， Dataframe就只是Dataset[Row]的一个别名，不再是一个单独的类
了。无论是typed方法（map、 filter、 groupByKey等）还是untyped方法（select、
groupBy等），都通过Dataset来提供。而且Dataset API将成为Spark的新一代流式计
算框架——structured streaming的底层计算引擎。但是由于Python和R这两个语言都
不具备compile-time type-safety的特性，所以就没有引入Dataset API，所以这两种语
言中的主要编程接口还是Dataframe。


SparkSession
SparkSession是新的Spark上下文以及入口，用于合并SQLContext和HiveContext，并替
代它们。因为以前提供了SQLContext和HiveContext两种上下文入口，因此用户有时
会有些迷惑，到底该使用哪个接口。现在好了，只需要使用一个统一的SparkSession
即可。但是为了向后兼容性， SQLContext和HiveContext还是保留下来了。


新版本Accumulator API
Spark 2.0提供了新版本的Accumulator，提供了各种方便的方法，比如说直接通过一
个方法的调用，就可以创建各种primitive data type（原始数据类型， int、 long、
double）的Accumulator。并且在spark web ui上也支持查看spark application的
accumulator，性能也得到了提升。老的Accumulator API还保留着，主要是为了向后
兼容性。


基于Dataframe/Dataset的
Spark MLlib
Spark 2.0中， spark.ml包下的机器学习API，主要是基于Dataframe/Dataset来实现的，
未来将会成为主要发展的API接口。原先老的基于RDD的spark.mllib包的机器学习API
还会保留着，为了向后兼容性，但是未来主要会基于spark.ml包下的接口来进行开
发。而且用户使用基于Dataframe/Dataset的新API，还能够对算法模型和pipeline进
行持久化保存以及加载。


SparkR中的分布式机器学习
算法以及UDF函数
Spark 2.0中，为SparkR提供了分布式的机器学习算法，包括经典的Generalized Linear
Model，朴素贝叶斯， Survival Regression， K-means等。此外SparkR还支持用户自定
义的函数，即UDF。

Spark 2.0-高性能：让Spark作为编译器来运行



让Spark作为编译器来运行
在一个2015年的spark调查中显示， 91%的spark用户是因为spark的高性能才选择使用它的。所
以spark的性能优化也就是社区的一个重要的努力方向了。 spark 1.x相较于hadoop mapreduce来
说，速度已经快了数倍了，但是spark 2.x中，还能不能相较于spark 1.x来说，速度再提升10倍
呢？
带着这个疑问，我们可以重新思考一下spark的物理执行机制。对于一个现代的大数据处理引
擎来说， CPU的大部分时间都浪费在了一些无用的工作上，比如说virtual function call，或者从
CPU缓冲区中读写数据。现代的编译器为了减少cpu浪费在上述工作的时间，付出了大量的努力。


让Spark作为编译器来运行
Spark 2.0的一个重大的特点就是搭载了最新的第二代tungsten引擎。第二代tungsten引擎吸取了
现代编译器以及并行数据库的一些重要的思想，并且应用在了spark的运行机制中。其中一个
核心的思想，就是在运行时动态地生成代码，在这些自动动态生成的代码中，可以将所有的操
作都打包到一个函数中，这样就可以避免多次virtual function call，而且还可以通过cpu register
来读写中间数据，而不是通过cpu cache来读写数据。上述技术整体被称作“whole-stage code
generation”，中文也可以叫“全流程代码生成”。


让Spark作为编译器来运行
之前有人做过测试，用单个cpu core来处理一行数据，对比了spark 1.6和spark 2.0的性能。 spark
2.0搭载的是whole-stage code generation技术， spark 1.6搭载的是第一代tungsten引擎的
expression code generation技术。测试结果显示， spark 2.0的性能相较于spark 1.6得到了一个数
量级的提升。


让Spark作为编译器来运行
除了刚才那个简单的测试以外，还有人使用完整的99个SQL基准测试来测试过spark 1.6和spark
2.0的性能。测试结果同样显示， spark 2.0的性能比spark 1.6来说，提升了一个数量级。


让Spark作为编译器来运行
spark 2.0中，除了whole-stage code generation技术以外，还使用了其他一些新技术来提升性能。
比如说对Spark SQL的catalyst查询优化器做了一些性能优化，来提升对一些常见查询的优化效率，
比如null值处理等。再比如说，通过vectarization技术将parquet文件扫描的吞吐量提升了3倍以
上。

Spark 2.0-新特性介绍-智能化：Structured Streaming
介绍



Structured Streaming介绍
Spark Streaming应该说是将离线计算操作和流式计算操作统一起来的大数据计算框架之一。从
Spark 0.7开始引入的Spark Streaming，为开发人员提供了很多有用的特性：一次且仅一次的语
义支持、容错性、强一致性保证、高吞吐量。
但是实际上在真正工业界的流式计算项目中，并不仅仅只是需要一个流式计算引擎。这些项目
实际上需要深度地使用批处理计算以及流式处理技术，与外部存储系统进行整合，还有应对业
务逻辑变更的能力。因此，企业实际上不仅仅只是需要一个流式计算引擎，他们需要的是一个
全栈式的技术，让他们能够开发end-to-end的持续计算应用（continuous application）。
Spark 2.0为了解决上述流式计算的痛点和需求，开发了新的模块——Structured Streaming。


Structured Streaming介绍
Structured Streaming提供了与批处理计算类似的API。要开发一个流式计算应用，开发人员只要使用
Dataframe/Dataset API编写与批处理计算一样的代码即可， Structured Streaming会自动将这些类似批处理的计
算代码增量式地应用到持续不断进入的新数据上。这样，开发人员就不需要花太多时间考虑状态管理、容错、
与离线计算的同步等问题。 Structured Streaming可以保证，针对相同的数据，始终与离线计算产出完全一样
的计算结果。
Structured Streaming还提供了与存储系统的事务整合。它会进行自动的容错管理以及数据一致性的管理，如
果开发人员要写一个应用程序来更新数据库，进而提供一些实时数据服务，与静态数据进行join，或者是在
多个存储系统之间移动数据，那么Structured Streaming可以让这些事情更加简单。
Structured Streaming与Spark其余的组件都能够进行完美的整合。比如可以通过Spark SQL对实时数据进行统计
分析，与静态数据进行join，还有其他的使用dataframe/dataset的组件，这样就可以让开发人员构建完整的流
式计算引用，而不仅仅只是一个流式计算引擎而已。在未来， Spark会将Structured Streaming与Spark MLlib的
整合做的更好。


Structured Streaming介绍
Spark 2.0搭载了一个beta版本的Structured Streaming，目前是作为Dataframe/Dataset的一个小的
附加组件。主要是让Spark用户可以先尝试使用一下Structured Streaming，比如做一些实验和测
试。 Structured Streaming的一些关键特性，比如基于时间的处理，延迟数据的处理，交互式的
查询，以及与非流式的数据源和存储进行整合，可能会基于未来的版本来实现。
Spark 2.0-新特性介绍-Spark 1.x的Volcano Iterator Model深度剖析



Volcano Iterator Model
深入剖析Spark 2.x的第二代tungsten引擎原理之前，先看一下当前的Spark的工作原理。我们可
以通过一个SQL来举例，这个SQL扫描了单个表，然后对属性等于指定值的记录进行汇总计数。
SQL语句如下： select count(*) from store_sales where ss_item_sk=1000。
要执行这个查询， Spark 1.x会使用一种最流行、最经典的查询求值策略，该策略主要基于
Volcano Iterator Model。在这种模型中，一个查询会包含多个operator，每个operator都会实现
一个接口，提供一个next()方法，该方法返回operator tree中的下一个operator。


Volcano Iterator Model
举例来说，上面那个查询中的filter operator的代码大致如下所示：
让每一个operator都实现一个iterator接口，
可以让查询引擎优雅的组装任意operator在
一起。而不需要查询引擎去考虑每个operator
具体的一些处理逻辑，比如数据类型等。
Vocano Iterator Model也因此成为了数据库SQL执行引擎领域内内的20年中最流行的一种标准。
而且Spark
SQL最初的SQL执行引擎也是基于这个思想来实现的。


Volcano Iterator Model
vs
手写代码
对于上面的那个查询，如果我们通过java来手工编写一段代码实现那个功能，代码大致如下所示：
上面这段代码是专门为实现这个指定的功能编写的，因此不具备良好的组装性
以及扩展性。那么Volcano Iterator Model与这段手写代码的性能对比是怎么样的
呢？一边是20年中最流行的一种SQL引擎思想，另一种是一段近乎小白编写的
简单代码。有人对这两种方式的性能做了一个实验和对比：


Volcano Iterator Model
vs
手写代码
我们可以清晰地看到，手写的代码的性能比Volcano Iterator Model高了一整个数量级，而这其中的原因包含以下几点：
1、避免了virtual function dispatch： 在Volcano Iterator Model中，至少需要调用一次next()函数来获取下一个operator。
这些函数调用在操作系统层面，会被编译为virtual function dispatch。而手写代码中，没有任何的函数调用逻辑。虽然
说，现代的编译器已经对虚函数调用进行了大量的优化，但是该操作还是会执行多个CPU指令，并且执行速度较慢，
尤其是当需要成百上千次地执行虚函数调用时。
2、通过CPU Register存取中间数据，而不是内存缓冲：在Volcano Iterator Model中，每次一个operator将数据交给下一
个operator，都需要将数据写入内存缓冲中。然而在手写代码中， JVM JIT编译器会将这些数据写入CPU Register。 CPU
从内存缓冲种读写数据的性能比直接从CPU Register中读写数据，要低了一个数量级。
3、 Loop Unrolling和SIMD： 现代的编译器和CPU在编译和执行简单的for循环时，性能非常地高。编译器通常可以自动
对for循环进行unrolling，并且还会生成SIMD指令以在每次CPU指令执行时处理多条数据。 CPU也包含一些特性，比如
pipelining， prefetching，指令reordering，可以让for循环的执行性能更高。然而这些优化特性都无法在复杂的函数调用
场景中施展，比如Volcano Iterator Model。


Volcano Iterator Model
vs
手写代码
loop unrolling解释（小白的方式）
for(int i = 0; i < 10; i++) { System.out.println(i) }
System.out.println(1)
System.out.println(2)
System.out.println(3)
......
手写代码的好处就在于，它是专门为实现这个功能而编写的，代码简单，因此可以吸收上述所
有优点，包括避免虚函数调用，将中间数据保存在CPU寄存器中，而且还可以被底层硬件进行
for循环的自动优化。

Spark 2.0-新特性介绍-whole-stage code generation技术和vectorization技术



Whole-stage code
generation
之前讲解了手工编写的代码的性能，为什么比Volcano Iterator Model要好。所以如果要对Spark
进行性能优化，一个思路就是在运行时动态生成代码，以避免使用Volcano模型，转而使用性
能更高的代码方式。要实现上述目的，就引出了Spark第二代Tungsten引擎的新技术， wholestage code generation。通过该技术， SQL语句编译后的operator-treee中，每个operator执行时就
不是自己来执行逻辑了，而是通过whole-stage code generation技术，动态生成代码，生成的代
码中会尽量将所有的操作打包到一个函数中，然后再执行动态生成的代码。


Whole-stage code
generation
就以上一讲的SQL语句来作为示例， Spark会自动生成以下代码。如果只是一个简单的查询，那
么Spark会尽可能就生成一个stage，并且将所有操作打包到一起。但是如果是复杂的操作，就
可能会生成多个stage。


Whole-stage code
generation
Spark提供了explain()方法来查看一个SQL的执行计划，而且这里面是可以看到通过whole-stage
code generation生成的代码的执行计划的。如果看到一个步骤前面有个*符号，那么就代表这个
步骤是通过该技术自动生成的。在这个例子中， Range、 Filter和Aggregation都是自动生成的，
Exchange不是自动生成的，因为这是一个网络传输数据的过程。


Whole-stage code
generation
很多用户会疑惑，从Spark 1.1版本开始，就一直听说有code generation类的feature引入，这跟
spark 2.0中的这个技术有什么不同呢。实际上在spark 1.x版本中， code generation技术仅仅被使
用在了expression evoluation方面（比如a + 1），即表达式求值，还有极其少数几个算子上（比
如filter等）。而spark 2.0中的whole-stage code generation技术是应用在整个spark运行流程上的。


Vectorization
对于很多查询操作， whole-stage code generation技术都可以很好地优化其性能。但是有一些特殊的操作，却无法很好
的使用该技术，比如说比较复杂一些操作，如parquet文件扫描、 csv文件解析等，或者是跟其他第三方技术进行整合。
如果要在上述场景提升性能， spark引入了另外一种技术，称作“vectorization”，即向量化。向量化的意思就是避免每次
仅仅处理一条数据，相反，将多条数据通过面向列的方式来组织成一个一个的batch，然后对一个batch中的数据来迭
代处理。每次next()函数调用都返回一个batch的数据，这样可以减少virtual function dispatch的开销。同时通过循环的
方式来处理，也可以使用编译器和CPU的loop unrolling等优化特性。


Vectorization
这种向量化的技术，可以使用到之前说的3个点中的2个点。即，减少virtual function dispatch，
以及进行loop unrolling优化。但是还是需要通过内存缓冲来读写中间数据的。所以，仅仅当实
在无法使用whole-stage code generation时，才会使用vectorization技术。有人做了一个parquet
文件读取的实验，采用普通方式以及向量化方式，性能也能够达到一个数量级的提升：


总结
上述的whole-stage code generation技术，能否保证将spark 2.x的性能比spark 1.x来说提升10倍以
上呢？这是无法完全保证的。虽然说目前的spark架构已经搭载了目前世界上最先进的性能优
化技术，但是并不是所有的操作都可以大幅度提升性能的。简单来说， CPU密集型的操作，可
以通过这些新技术得到性能的大幅度提升，但是很多IO密集型的操作，比如shuffle过程的读写
磁盘，是无法通过该技术提升性能的。在未来， spark会花费更多的精力在优化IO密集型的操作
的性能上。
Spark 2.0-Spark 2.x与1.x对比以及分析、学习建议以及使用建
议



Spark 2.x与1.x对比
Spark 1.x： Spark Core（RDD）、 Spark SQL（SQL+Dataframe+Dataset）、 Spark Streaming、 Spark
MLlib、 Spark Graphx
Spark 2.x： Spark Core（RDD）、 Spark SQL（ANSI-SQL+Subquery+Dataframe/Dataset）、 Spark
Streaming、 Structured Streaming、 Spark MLlib（Dataframe/Dataset）、 Spark Graphx、 Second
Generation Tungsten Engine（Whole-stage code generation+Vectorization）


Spark 2.x与1.x对比
这里首先给大家理清楚一个前提： Spark 1.x到Spark 2.x，完全是一脉相承的关系，
即， Spark 2.x基本上是基于Spark 1.x进行了更多的功能和模块的扩展，以及底层性
能的改良。绝对不是说， Spark 2.x彻底淘汰和替代了Spark 1.x中的组件。而且实际
上，对于Spark 1.x中90%以上的东西， Spark 2.x几乎都完全保留了支持和延续，并没
有做任何改变。这是大家必须要了解的一件事情。
下面我们就对Spark 2.x中的每个组件都进行分析，告诉大家这些组件的基本原理，
以及其适用和不适用的场景。避免大家对Spark 1.x和Spark 2.x有错误的认知。


Spark 2.x各组件分析
Spark Core（RDD）
从Spark诞生之日开始， RDD就是Spark最主要的编程接口，重要程度类似于Hadoop中的MapReduce。 RDD，简单来说，
就是一个不可变的分布式数据集，被分为多个partition从而在一个集群上分布式地存储。我们可以使用RDD提供的各种
transformation和action算子，对RDD执行分布式的计算操作。
可能很多人会问， Spark 2.0开始，包括Structured Streaming、 Spark MLlib、 Spark SQL底层都开始基于Dataframe/Dataset
来作为基础计算引擎，那么Spark Core/RDD是不是就要被淘汰了？
回答是：错误！
Spark官方社区对于这个问题也是这个态度， Spark Core绝对不会被淘汰掉。因为Spark Core/RDD作为一种low-level的API
有它的较为底层的应用场景，虽然后续这种场景会越来越少， Dataframe/Dataset API会逐渐替代原先Spark Core的一些
场景，但是不可否认的是，这种场景还是存在的。此外， Dataframe/Dataset实际上底层也是基于Spark Core/RDD构建的。
所以说， Spark Core/RDD是Spark生态中，不可替代的基础API和引擎，其他所有的组件几乎都是构建在它之上。
未来它不会被淘汰，只是应用场景会减少而已。


Spark 2.x各组件分析
Spark Core（RDD）
Spark 2.x中，在离线批处理计算中，编程API，除了RDD以外，还增强了Dataframe/Dataset API。那么，我们到底什么时
候应该使用Spark Core/RDD来进行编程呢？实际上， RDD和Dataset最大的不同在于， RDD是底层的API和内核， Dataset
实际上基于底层的引擎构建的high-level的计算引擎。
1、如果我们需要对数据集进行非常底层的掌控和操作，比如说，手动管理RDD的分区，或者根据RDD的运行逻辑来结
合各种参数和编程来进行较为底层的调优。因为实际上Dataframe/Dataset底层会基于whole-stage code generation技术
自动生成很多代码，那么就意味着，当我们在进行线上报错的troubleshooting以及性能调优时，对Spark的掌控能力就
会降低。而使用Spark Core/RDD，因为其运行完全遵循其源码，因此我们完全可以在透彻阅读Spark Core源码的基础之
上，对其进行troubleshooting和底层调优。 （最重要的一点）
2、我们要处理的数据是非结构化的，比如说多媒体数据，或者是普通文本数据。
3、我们想要使用过程式编程风格来处理数据，而不想使用domain-specific language的编程风格来处理数据。
4、我们不关心数据的schema，即元数据。
5、我们不需要Dataframe/Dataset底层基于的第二代tungsten引擎提供的whole-stage code generation等性能优化技术。


Spark 2.x各组件分析
Spark SQL（ANSI-SQL+Subquery）
Spark 2.x中的Spark SQL，提供了标准化SQL的支持，以及子查询的支持，大幅度提升了Spark在SQL领域的应用场景。而
且本身在大数据领域中， SQL就是一个最广泛使用的用户入口，据不完全统计以及讲师的行业经验，做大数据的公司
里， 90%的应用场景都是基于SQL的。最典型的例子就是Hadoop，几乎用Hadoop的公司， 90%都是基于Hive进行各种大
数据的统计和分析。剩下10%是实时计算、机器学习、图计算。之所以有这种现象，主要就是因为SQL简单、易学、易
用、直观。无论是研发人员，还是产品经理，还是运营人员，还是其他的人，都能在几天之内入门和学会SQL的使用，
然后就可以基于大数据SQL引擎（比如Hive）基于企业积累的海量数据，根据自己的需求进行各种统计和分析。
此外，据Spark官方社区所说， Spark 2.x一方面对SQL的支持做了大幅度的增强，另一方面，也通过优化了底层的计算
引擎（第二代tungsten引擎， whole-stage code generation等），提升了SQL的执行性能以及稳定性。
所以在Spark 2.x中，一方面，开始鼓励大家多使用Spark SQL的SQL支持，采用Spark SQL来编写SQL进行最常见的大数据
统计分析。比如可以尝试将Hive中的运行的一些SQL语句慢慢迁移到Spark SQL上来。另外一方面，也提醒大家，一般一
个新的大版本，都是不太稳定的，因此Spark SQL虽然在功能、性能和稳定性上做了很多的增强，但是难免还是会有很
多的坑。因此建议大家在做Hive/RDBMS（比如Oracle）到Spark SQL的迁移时，要小心谨慎，一点点迁移，同时做好踩
坑的准备。


Spark 2.x各组件分析
Spark SQL（Dataframe/Dataset）
就像RDD一样， Dataframe也代表一个不可变的分布式数据集。与RDD不同的一点是， Dataframe引入了schema的概念，
支持以复杂的类型作为元素类型，同时指定schema，比如Row。因此Dataframe更像是传统关系型数据库中的表的概念。
为了提升开发人员对大数据的处理能力， Dataframe除了提供schema的引入，还基于Schema提供了很多RDD所不具备的
high-level API，以及一些domain-specific language（特定领域编程语言）。但是在Spark 2.0中， Dataframe和Dataset合并
了， Dataframe已经不是一个单独的概念了，目前仅仅只是Dataset[Row]的一个类型别名而已，你可以理解为Dataframe
就是Dataset。
Datafram
e
Dataset
Dataset
（Spark 2.0）
Typed
API
•Dataset[T]
Untyped
API
•Dataframe = Dataset[Row]
•Alias


Spark 2.x各组件分析
Spark SQL（Dataframe/Dataset）
从Spark 2.0开始， Dataset有两种表现形式： typed API和untyped API。我们可以认为， Dataframe就是Dataset[Row]的别
名， Row就是一个untyped类型的对象，因为Row是类似于数据库中的一行，我们只知道里面有哪些列，但是有些列即
使不存在，我们也可以这对这些不存在的列进行操作。因此其被定义为untyped，就是弱类型。
而Dataset[T]本身，是一种typed类型的API，其中的Object通常都是我们自己自定义的typed类型的对象，因为对象是我
们自己定义的，所以包括字段命名以及字段类型都是强类型的。目前Scala支持Dataset和Dataframe两种类型， Java仅仅
支持Dataset类型， Python和R因为不具备compile-time type-safety特性，因此仅仅支持Dataframe。


Spark 2.x各组件分析
Spark SQL（Dataframe/Dataset）
Dataset API有哪些优点呢？
1、静态类型以及运行时的类型安全性
SQL语言具有最不严格的限制，而Dataset具有最严格的限制。 SQL语言在只有在运行时才能发现一些错误，比如类型错
误，但是由于Dataframe/Dataset目前都是要求类型指定的（静态类型），因此在编译时就可以发现类型错误，并提供
运行时的类型安全。比如说，如果我们调用了一个不属于Dataframe的API，编译时就会报错。但是如果你使用了一个
不存在的列，那么也只能到运行时才能发现了。而最严格的就是Dataset了，因为Dataset是完全基于typed API来设计的，
类型都是严格而且强类型的，因此如果你使用了错误的类型，或者对不存在的列进行了操作，都能在编译时就发现。
SQL 	Dataframe 	Dataset
Syntax
Error	Runtime 	Compile
Time	Compile
Time
Analysis
Error	Runtime 	Runtime 	Compile
Time




Spark 2.x各组件分析
Spark SQL（Dataframe/Dataset）
2、将半结构化的数据转换为typed自定义类型
举例来说，如果我们现在有一份包含了学校中所有学生的信息，是以JSON字符串格式定义的，比如： {“name”: “leo”,
“age”, 19, “classNo”: 1}。我们可以自己定义一个类型，比如case class Student(name: String, age: Integer, classNo: Integer)。
接着我们就可以加载指定的json文件，并将其转换为typed类型的Dataset[Student]，比如val ds =
spark.read.json("students.json").as[Student]。
在这里， Spark会执行三个操作：
1、 Spark首先会读取json文件，并且自动推断其schema，然后根据schema创建一个Dataframe。
2、在这里，会创建一个Dataframe=Dataset[Row]，使用Row来存放你的数据，因为此时还不知道具体确切的类型。
3、接着将Dataframe转换为Dataset[Student]，因为此时已经知道具体的类型是Student了。
这样，我们就可以将半结构化的数据，转换为自定义的typed结构化强类型数据集。并基于此，得到之前说的编译时和
运行时的类型安全保障。


Spark 2.x各组件分析
Spark SQL（Dataframe/Dataset）
3、 API的易用性
Dataframe/Dataset引入了很多的high-level API，并提供了domain-specific language风格的编程接口。这样的话，大部分
的计算操作，都可以通过Dataset的high-level API来完成。通过typed类型的Dataset，我们可以轻松地执行agg、 select、
sum、 avg、 map、 filter、 groupBy等操作。使用domain-specific language也能够轻松地实现很多计算操作，比如类似RDD
算子风格的map()、 filter()等。
4、性能
除了上述的优点， Dataframe/Dataset在性能上也有很大的提升。首先， Dataframe/Dataset是构建在Spark SQL引擎之上
的，它会根据你执行的操作，使用Spark SQL引擎的Catalyst来生成优化后的逻辑执行计划和物理执行计划，可以大幅度
节省内存或磁盘的空间占用的开销（相对于RDD来说， Dataframe/Dataset的空间开销仅为1/3~1/4），也能提升计算的
性能。其次， Spark 2.x还引入第二代Tungsten引擎，底层还会使用whole-stage code generation、 vectorization等技术来优
化性能。


Spark 2.x各组件分析
Spark SQL（Dataframe/Dataset）
什么时候应该使用Dataframe/Dataset，而不是RDD呢？
1、如果需要更加丰富的计算语义， high-level的抽象语义，以及domain-specific API。
2、如果计算逻辑需要high-level的expression、 filter、 map、 aggregation、 average、 sum、 SQL、列式存储、 lambda表达
式等语义，来处理半结构化，或结构化的数据。
3、如果需要高度的编译时以及运行时的类型安全保障。
4、如果想要通过Spark SQL的Catalyst和Spark 2.x的第二代Tungsten引擎来提升性能。
5、如果想要通过统一的API来进行离线、流式、机器学习等计算操作。
6、如果是R或Python的用户，那么只能使用Dataframe。
最后，实际上， Spark官方社区对RDD和Dataframe/Dataset的建议时，按照各自的特点，根据的需求场景，来灵活的选
择最合适的引擎。甚至说，在一个Spark应用中，也可以将两者结合起来一起使用。


Spark 2.x各组件分析
Spark Streaming&Structured Streaming
Spark Streaming是老牌的Spark流式计算引擎，底层基于RDD计算引擎。除了类似RDD风格的计算API以外，也提供了更
多的流式计算语义，比如window、 updateStateByKey、 transform等。同时对于流式计算中重要的数据一致性、容错性
等也有一定的支持。
但是Spark 2.x中也推出了全新的基于Dataframe/Dataset的Structured Streaming流式计算引擎。相较于Spark Streaming来
说，其最大的不同之处在于，采用了全新的逻辑模型，提出了real-time incremental table的概念，更加统一了流式计算
和离线计算的概念，减轻了用户开发的负担。同时还提供了（可能在未来提供）高度封装的特性，比如双流的全量
join、与离线数据进行join的语义支持、内置的自动化容错机制、内置的自动化的一次且仅一次的强一致性语义、 timebased processing、延迟数据达到的自动处理、与第三方外部存储进行整合的sink概念，等等高级特性。大幅度降低了
流式计算应用的开发成本。
这里要提的一句是，首先，目前暂时建议使用Spark Streaming，因为Spark Streaming基于RDD，而且经过过个版本的考
验，已经趋向于稳定。对于Structured Streaming来说，一定要强调，在Spark 2.0版本刚推出的时候，千万别在生产环境
使用，因为目前官方定义为beta版，就是测试版，里面可能有很多的bug和问题，而且上述的各种功能还不完全，很多
功能还没有。因此Structured Streaming的设计理念虽然非常好，但是个人建议在后续的版本中再考虑使用。目前可以
保持关注和学习，并做一些实验即可。


Spark 2.x各组件分析
Spark MLlib&GraphX
Spark MLlib未来将主要基于Dataframe/Dataset API来开发。而且还会提供更多的机器学习算法。因此可以主要考虑使用
其spark.ml包下的API即可。
Spark GraphX，目前发展较为缓慢，如果有图计算相关的应用，可以考虑使用。


Spark 2.x学习建议
纵观之前讲的内容， Spark 2.0本次，其实主要就是提升了底层的性能，搭载了第二代Tungsten引擎；同时大幅度调整和
增强了ANSI-SQL支持和Dataframe/Dataset API，并将该API作为Spark未来重点发展的发现；此外，为了提供更好的流式
计算解决方案，发布了一个测试版的Structured Streaming模块。
而且之前也讲解了Spark 1.x和Spark 2.x中的每一个模块。大家可以明确看到：
第一， Spark 1.x没有任何一个组件是被淘汰的；
第二， Spark这次重点改造的是Tungsten Engine、 Dataframe/Dataset以及Structured Streaming，对于之前Spark 1.x课程中
讲解的Spark Core、 Spark SQL以及Spark Streaming，包括Spark Core的性能调优和源码剖析，集群运维管理，几乎没有做
太多的调整；
第三， Spark Core、 Spark SQL、 Spark Streaming、 Dataframe/Dataset、 Structured Streaming、 Spark MLlib和GraphX，每个
组件目前都有其特点和用途，任何一个不是积累和过时的技术；
第五， Spark 2.0的新东西中， ANSI-SQL和Dataframe/Dataset API是可以重点尝试使用的，但是Structured Streaming还停
留在实验阶段，完全不能应用到生产项目中。因此目前流式计算主要还是使用Spark Streaming。个人预计，至少要在
2017年春节过后， Structured Streaming才有可能进入稳定状态，可以尝试使用。


Spark 2.x学习建议
首先，对于课程之前讲解的Spark 1.x的所有知识，目前以及之后可预见的时间范围内，都是一直有价值的，都是需要
学习的。无论是Spark Core（RDD）编程，作为整个Spark生态的基石（包括Dataframe/Dataset），以及掌握Spark底层的
知识；还是Spark SQL的开发，或者是Spark Streaming的开发；还有它们的性能调优、 Spark Core源码剖析；以及管理运
维，这些知识都没有过时，都是价值的，大家都必须认真、仔细的学习，绝对不能轻浮冒进，直接就简单学学
Dataframe/Dataset， Structured Streaming，就以为自己掌握了Spark 2.x了，那是绝对错误的！
本次课程升级，主要分为三个阶段，第一个阶段就是Spark 2.x的新特性介绍，主要包括了新特性概览、发展方向、核
心原理以及与1.x的对比分析、学习建议以及使用建议；第二个阶段就是Dataset的开发详解；第三个阶段就是
Structured Streaming开发详解。因此在透彻掌握Spark 1.x的基础之上，再来学习Spark 2.x效果更佳。其中最重要的，是
要掌握Spark第二代Tungsten引擎的性能提升原理、 Spark ANSI-SQL和子查询的支持、 Dataset的开发以及使用、
Structured Streaming的开发以及使用。


Spark 2.x使用建议
在透彻学习了Spark 1.x和Spark 2.x的知识体系之后，对于Spark的使用，建议如下
1、建议开始大量尝试使用Spark SQL的标准化SQL支持以及子查询支持的特性，大部分的大数据统计分析应用，采用
Spark SQL来实现。
2、其次，对于一些无法通过SQL来实现的复杂逻辑，比如一些算法的实施，或者一些跟DB、缓存打交道的大数据计算
应用，建议采用Dataframe/Dataset API来实施。
3、接着，对于一些深刻理解课程中讲解的Spark Core/RDD，以及内核源码的高阶同学，如果遇到了因为Spark SQL和
Dataframe/Dataset导致的线上的莫名其妙的报错，始终无法解决，或者是觉得有些性能，通过第二代Tungsten引擎也无
法很好的调优，需要自己手工通过RDD控制底层的分区以及各种参数来进行调优，那么建议使用Spark Core/RDD来重写
SQL类应用。
4、对于流式计算应用，建议目前还是使用Spark Streaming，因为其稳定； Structured Streaming目前是beta版本，很不
稳定，因此目前建议仅仅是学习和实验即可。个人预计和建议，估计至少要到2017年春节后， Structured Streaming才
可能具备部署生产环境的能力。
5、对于机器学习应用，建议使用spark.ml包下的机器学习API，因为其基于Dataframe/Dataset API实现，性能更好，而
且未来是社区重点发展方向


Spark 2.0-课程环境搭建：虚拟机、 CentOS、 Hadoop、 Spark等



CentOS 6.5集群搭建
虚拟机安装
1、使用课程提供的CentOS 6.5镜像即可， CentOS-6.5-i386-minimal.iso。
2、创建虚拟机：打开Virtual Box，点击“新建”按钮，点击“下一步”，输入虚拟机名称为
spark2upgrade01，选择操作系统为Linux，选择版本为Red Hat，分配4096MB内存，后面的选项全部用默
认，在Virtual Disk File location and size中，一定要自己选择一个目录来存放虚拟机文件，最后点击“create”
按钮，开始创建虚拟机。
3、设置虚拟机网卡：选择创建好的虚拟机，点击“设置”按钮，在网络一栏中，连接方式中，选择
“Bridged Adapter”。
4、安装虚拟机中的CentOS 6.5操作系统：选择创建好的虚拟机，点击“开始”按钮，选择安装介质（即本
地的CentOS 6.5镜像文件），选择第一项开始安装-Skip-欢迎界面Next-选择默认语言-Baisc Storage
Devices-Yes, discard any data-主机名:spark2upgrade01-选择时区-设置初始密码为hadoop-Replace
Existing Linux System-Write changes to disk-CentOS 6.5自己开始安装。
5、安装完以后， CentOS会提醒你要重启一下，就是reboot，你就reboot就可以了。


CentOS 6.5集群搭建
配置网络
vi /etc/sysconfig/network-scripts/ifcfg-eth0
DEVICE=eth0
TYPE=Ethernet
ONBOOT=yes
BOOTPROTO=dhcp
service network restart
ifconfig
BOOTPROTO=static
IPADDR=192.168.0.X
NETMASK=255.255.255.0
GATEWAY=192.168.0.1
service network restart
vi /etc/hosts
配置本机的hostname到ip地址的映射
此时就可以使用SecureCRT从本机连接到虚拟机进行操作了


CentOS 6.5集群搭建
关闭防火墙
service iptables stop
service ip6tables stop
chkconfig iptables off
chkconfig ip6tables off
vi /etc/selinux/config
SELINUX=disabled
关闭windows的防火墙


CentOS 6.5集群搭建
安装yum
yum clean all
yum makecache
yum install telnet


CentOS 6.5集群搭建
安装JDK 1.7
1、将jdk-7u60-linux-i586.rpm通过WinSCP上传到虚拟机中
2、安装JDK： rpm -ivh jdk-7u65-linux-i586.rpm
3、配置jdk相关的环境变量
vi .bashrc
export JAVA_HOME=/usr/java/latest
export PATH=$PATH:$JAVA_HOME/bin
source .bashrc
4、测试jdk安装是否成功： java -version


CentOS 6.5集群搭建
安装另外两台虚拟机
1、按照上述步骤，再安装两台一模一样环境的虚拟机，唯一的区别是内存为1024MB。
2、另外两台机器的hostname分别设置为spark2upgrade02和spark2upgrade03即可。
3、在安装的时候，另外两台虚拟机的centos镜像文件必须重新拷贝一份，使用自己的镜像文
件。
4、虚拟机的硬盘文件也重新选择一个新的目录。
5、安装好之后，要在三台机器的/etc/hosts文件中，配置全三台机器的ip地址到hostname的映
射 6
、在windows的hosts文件中也要配置全三台机器的ip地址到hostname的映射。


CentOS 6.5集群搭建
配置集群的SSH免密码通信
1、首先在三台机器上配置对本机的ssh免密码登录
ssh-keygen -t rsa
生成本机的公钥，过程中不断敲回车即可， ssh-keygen命令默认会将公钥放在/root/.ssh目录
下
cd /root/.ssh
cp id_rsa.pub authorized_keys
将公钥复制为authorized_keys文件，此时使用ssh连接本机就不需要输入密码了
2、接着配置三台机器互相之间的ssh免密码登录
使用ssh-copy-id -i spark命令将本机的公钥拷贝到指定机器的authorized_keys文件中


Hadoop 2.4集群搭建
部署hadoop安装包
1、使用课程提供的hadoop-2.4.1.tar.gz，使用WinSCP上传到CentOS的/usr/local目录下。
2、将hadoop包进行解压缩： tar -zxvf hadoop-2.4.1.tar.gz
3、对hadoop目录进行重命名： mv hadoop-2.4.1 hadoop
4、配置hadoop相关环境变量
vi .bashrc
export HADOOP_HOME=/usr/local/hadoop
export PATH=$HADOOP_HOME/bin:$HADOOP_HOME/sbin
source .bashrc
5、在/usr/local目录下创建data目录


Hadoop 2.4集群搭建
修改core-site.xml配置文件
<property>
<name>fs.default.name</name>
<value>hdfs://spark2upgrade01:9000</value>
</property>


Hadoop 2.4集群搭建
修改hdfs-site.xml配置文件
<property>
<name>dfs.name.dir</name>
<value>/usr/local/data/namenode</value>
</property>
<property>
<name>dfs.data.dir</name>
<value>/usr/local/data/datanode</value>
</property>
<property>
<name>dfs.tmp.dir</name>
<value>/usr/local/data/tmp</value>
</property>
<property>
<name>dfs.replication</name>
<value>3</value>
</property>


Hadoop 2.4集群搭建
修改mapred-site.xml配置文件
<property>
<name>mapreduce.framework.name</name>
<value>yarn</value>
</property>


Hadoop 2.4集群搭建
修改yarn-site.xml配置文件
<property>
<name>yarn.resourcemanager.hostname</name>
<value>spark2upgrade01</value>
</property>
<property>
<name>yarn.nodemanager.aux-services</name>
<value>mapreduce_shuffle</value>
</property>


Hadoop 2.4集群搭建
修改slaves配置文件
spark2upgrade01
spark2upgrade02
spark2upgrade03


Hadoop 2.4集群搭建
在另外两台机器上部署hadoop
1、使用scp命令将spark2upgrade01上面的hadoop安装包和.bashrc配置文件都拷
贝过去。
2、要记得对.bashrc文件进行source，以让它生效。
3、记得在另外两台机器的/usr/local目录下创建data目录。


Hadoop 2.4集群搭建
启动hdfs集群
1、格式化namenode：在spark2upgrade01上执行以下命令hdfs namenode -
format
2、启动hdfs集群： start-dfs.sh
3、验证启动是否成功： jps、 50070端口
spark2upgrade01： namenode、 datanode、 secondarynamenode
spark2upgrade02： datanode
spark2upgrade03： datanode


Hadoop 2.4集群搭建
启动yarn集群
1、启动yarn集群： start-yarn.sh
2、验证启动是否成功： jps、 8088端口
spark2upgrade01： resourcemanager、 nodemanager
spark2upgrade02： nodemanager
spark2upgrade03： nodemanager


Hive 0.13搭建
部署hive安装包
1、将课程提供的apache-hive-0.13.1-bin.tar.gz使用WinSCP上传到spark1的
/usr/local目录下。
2、解压缩hive安装包： tar -zxvf apache-hive-0.13.1-bin.tar.gz。
3、重命名hive目录： mv apache-hive-0.13.1-bin hive
4、配置hive相关的环境变量
vi .bashrc
export HIVE_HOME=/usr/local/hive
export PATH=$HIVE_HOME/bin
source .bashrc


Hive 0.13搭建
安装mysql
1、在spark2upgrade01上安装mysql。
2、使用yum安装mysql server。
yum install -y mysql-server
service mysqld start
chkconfig mysqld on
3、使用yum安装mysql connector
yum install -y mysql-connector-java
4、将mysql connector拷贝到hive的lib包中
cp /usr/share/java/mysql-connector-java-5.1.17.jar /usr/local/hive/lib
5、在mysql上创建hive元数据库，并对hive进行授权
create database if not exists hive_metadata;
grant all privileges on hive_metadata.* to 'hive'@'%' identified by 'hive';
grant all privileges on hive_metadata.* to 'hive'@'localhost' identified by 'hive';
grant all privileges on hive_metadata.* to 'hive'@'spark2upgrade01' identified by 'hive';
flush privileges;
use hive_metadata;


Hive 0.13搭建
修改hive-site.xml配置文件
mv hive-default.xml.template hive-site.xml
vi hive-site.xml
<property>
<name>javax.jdo.option.ConnectionURL</name>
<value>jdbc:mysql://spark2upgrade01:3306/hive_metadata?createDatabaseIfNotExist=true</value>
</property>
<property>
<name>javax.jdo.option.ConnectionDriverName</name>
<value>com.mysql.jdbc.Driver</value>
</property>
<property>
<name>javax.jdo.option.ConnectionUserName</name>
<value>hive</value>
</property>
<property>
<name>javax.jdo.option.ConnectionPassword</name>
<value>hive</value>
</property>


Hive 0.13搭建
配置hive-env.sh和hive-config.sh
mv hive-env.sh.template hive-env.sh
vi /usr/local/hive/bin/hive-config.sh
export JAVA_HOME=/usr/java/latest
export HIVE_HOME=/usr/local/hive
export HADOOP_HOME=/usr/local/hadoop


Hive 0.13搭建
验证安装是否成功
直接输入hive命令，可以进入hive命令行


Spark 2.0集群搭建
安装scala 2.11
1、将课程提供的scala-2.11.4.tgz使用WinSCP拷贝到spark1的/usr/local目录下。
2、对scala-2.11.4.tgz进行解压缩： tar -zxvf scala-2.11.4.tgz。
3、对scala目录进行重命名： mv scala-2.11.4 scala
4、配置scala相关的环境变量
vi .bashrc
export SCALA_HOME=/usr/local/scala
export PATH=$SCALA_HOME/bin
source .bashrc
5、查看scala是否安装成功： scala -version
6、在另外两台机器上都安装scala，使用scp将scala和.bashrc拷贝到过去即可。


Spark 2.0集群搭建
部署spark 2.0安装包
1、将spark-2.0.0-bin-hadoop2.4.tgz使用WinSCP上传到/usr/local目录下。自己在
http://spark.apache.org/downloads.html上下载即可。
2、解压缩spark包： tar zxvf spark-2.0.0-bin-hadoop2.4.tgz。
3、更改spark目录名： mv spark-2.0.0-bin-hadoop2.4.tgz spark
4、设置spark环境变量
vi .bashrc
export SPARK_HOME=/usr/local/spark
export PATH=$SPARK_HOME/bin
export CLASSPATH=.:$CLASSPATH:$JAVA_HOME/lib:$JAVA_HOME/jre/lib
source .bashrc


Spark 2.0集群搭建
配置spark-env.sh
1、 cd /usr/local/spark/conf
2、 cp spark-env.sh.template spark-env.sh
3、 vi spark-env.sh
export JAVA_HOME=/usr/java/latest
export SCALA_HOME=/usr/local/scala
export SPARK_MASTER_HOST=spark2upgrade01
export SPARK_WORKER_MEMORY=500m
export HADOOP_CONF_DIR=/usr/local/hadoop/etc/hadoop


Spark 2.0集群搭建
配置slaves
spark2upgrade01
spark2upgrade02
spark2upgrade03


Spark 2.0集群搭建
在另外两台机器上部署spark 2.0
在另外两个节点进行一模一样的配置，使用scp将spark和.bashrc拷贝到过去即可。


Spark 2.0集群搭建
配置spark可以使用hive
1、将hive-site.xml放置到spark的conf目录下
2、修改spark/conf和hive/conf下的hive-site.xml
<property>
<name>hive.metastore.uris</name>
<value>thrift://spark2upgrade01:9083</value>
</property>
3、启动hive metastore service
hive --service metastore &
4、 cp hive/lib/mysql-connector-java-5.1.17.jar spark/jars/
5、 hdfs dfs -chmod 777 /tmp/hive-root


Spark 2.0集群搭建
启动spark集群
1、在spark目录下的sbin目录
2、执行./start-all.sh
3、使用jsp和8080端口可以检查集群是否启动成功
4、进入spark-shell查看是否正常


Spark 2.0集群搭建
检查spark集群能否与hdfs整合使用
1、使用 spark-shell --master spark://spark2upgrade01:7077 --driver-memory 500m --executor-memory
500m启动
2、手工创建一份文件，上传到hdfs上去，放在/test_data/wordcount.txt中
3、在spark-shell中，基于hdfs上的文件，编写与运行一个wordcount程序
val lines = sc.textFile(“hdfs://spark2upgrade01:9000/test_data/wordcount.txt”)
val words = lines.flatMap(_.split(“ ”))
val pairs = words.map((_, 1))
val wordCounts = pairs.reduceByKey(_ + _)
wordCounts.collect().foreach(println(_))
4、观察是否有正确的结果
5、在spark web ui中检查是否有运行的作业记录


Spark 2.0集群搭建
检查spark集群能否与hive整合使用
1、创建一份文件， students.txt，每行是一个学生的信息
2、 CREATE TABLE students(name string, age int, score double)
3、 LOAD DATA LOCAL INPATH '/usr/local/test_data/students.txt' INTO TABLE students
4、 spark-shell --master spark://spark2upgrade01:7077 --driver-memory 500m --executor-memory 500m
5、在spark-shell中，运行针对hive的sql语句
spark.sql(“select * from students”).show();
spark.sql(“select name from students where score>=90;”).show();
spark.sql(“select name from students where age<=15;”).show();
6、观察是否有正确的结果
7、在spark web ui中检查是否有运行的作业记录


司


Spark 2.0-开发环境搭建：Eclipse+Maven+Scala+Spark



Spark开发环境
Maven
scala 2.11
Eclipse
安装eclipse scala插件： http://download.scala-ide.org/sdk/helium/e38/scala211/stable/site
安装eclipse m2e connector插件： http://download.eclipse.org/technology/m2e/releases
add scala library
Spark 2.0 Dependency


Spark开发环境
手动创建一个课程的工程，作为演示
maven project
org.scala-tools.archetypes
scala-archetype-simple
1.2
<inceptionYear>2015</inceptionYear>
<properties>
<scala.version>2.11.7</scala.version>
</properties>
Spark 2.0-SparkSession、 Dataframe、 Dataset开发入门



Spark SQL介绍
Spark SQL是Spark的一个模块，主要用于处理结构化的数据。与基础的Spark RDD API不同的是，
Spark SQL的接口会向提供更多的信息，包括数据结构以及要执行的计算操作等。在Spark SQL内
部，会使用这些信息执行一些额外的优化。使用Spark SQL有两种方式，包括SQL语句以及
Dataset API。但是在计算的时候，无论你是用哪种接口去进行计算，它们使用的底层执行引擎
是完全一模一样的。这种底层执行机制的统一，就意味着我们可以在不同的方式之间任意来回
切换，只要我们可以灵活地运用不同的方式来最自然地表达我们要执行的计算操作就可以了。


Spark SQL乊SQL介绍
Spark SQL的一个主要的功能就是执行SQL查询语句。 Spark 2.0开始，最大的一个改变，就是支
持了SQL 2003标准语法，还有就是支持子查询。 Spark SQL也可以用来从Hive中查询数据。当我
们使用某种编程语言开发的Spark作业来执行SQL时，返回的结果是Dataframe/Dataset类型的。
当然，我们也可以通过Spark SQL的shell命令行工具，或者是JDBC/ODBC接口来访问。


Spark SQL乊
Dataframe/Dataset介绍
Dataset是一个分布式的数据集。 Dataset是Spark 1.6开始新引入的一个接口，它结合了RDD API的很多优点（包括强类型，
支持lambda表达式等），以及Spark SQL的优点（优化后的执行引擎）。 Dataset可以通过JVM对象来构造，然后通过
transformation类算子（map， flatMap， filter等）来进行操作。 Scala和Java的API中支持Dataset，但是Python不支持
Dataset API。不过因为Python语言本身的天然动态特性， Dataset API的不少feature本身就已经具备了（比如可以通过
row.columnName来直接获取某一行的某个字段）。 R语言的情况跟Python也很类似。
Dataframe就是按列组织的Dataset。在逻辑概念上，可以大概认为Dataframe等同于关系型数据库中的表，或者是
Python/R语言中的data frame，但是在底层做了大量的优化。 Dataframe可以通过很多方式来构造：比如结构化的数据
文件， Hive表，数据库，已有的RDD。 Scala， Java， Python， R等语言都支持Dataframe。在Scala API中， Dataframe就是
Dataset[Row]的类型别名。在Java中，需要使用Dataset<Row>来代表一个Dataframe。


SparkSession：新的入口
从Spark 2.0开始，一个最大的改变就是， Spark SQL的统一入口就是SparkSession， SQLContext和HiveContext未来会被淘
汰。可以通过SparkSession.builder()来创建一个SparkSession，如下代码所示。 SparkSession内置就支持Hive，包括使用
HiveQL语句查询Hive中的数据，使用Hive的UDF函数，以及从Hive表中读取数据等。
val spark = SparkSession
.builder()
.appName("Spark SQL Example")
.master("local")
.config("spark.sql.warehouse.dir", "C:\\Users\\Administrator\\Desktop\\spark-warehouse")
.getOrCreate()
import spark.implicits._


Dataframe：untyped操作
有了SparkSession之后，就可以通过已有的RDD， Hive表，或者其他数据源来创建Dataframe，比如说通过json文件来创
建。 Dataframe提供了一种domain-specific language来进行结构化数据的操作，这种操作也被称之为untyped操作，与之
相反的是基于强类型的typed操作。
val df = spark.read.json("people.json")
df.show()
df.printSchema()
df.select("name").show()
df.select($"name", $"age" + 1).show()
df.filter($"age" > 21).show()
df.groupBy("age").count().show()


SparkSession：运行SQL查
询
SparkSession的sql()函数允许我们执行SQL语句，得到的结果是一个Dataframe。
df.createOrReplaceTempView("people")
val sqlDF = spark.sql("SELECT * FROM people")
sqlDF.show()


Dataset：typed操作
Dataset与RDD比较类似，但是非常重要的一点不同是， RDD的序列化机制是基于Java序列化机制或者是Kryo的，而Dataset的序列化机制基于一
种特殊的Encoder，来将对象进行高效序列化，以进行高性能处理或者是通过网络进行传输。 Dataset除了Encoder，也同时支持Java序列化机制，
但是encoder的特点在于动态的代码生成，同时提供一种特殊的数据格式，来让spark不将对象进行反序列化，即可直接基于二进制数据执行一
些常见的操作，比如filter、 sort、 hash等。
case class Person(name: String, age: Long)
val caseClassDS = Seq(Person("Andy", 32)).toDS()
caseClassDS.show()
val primitiveDS = Seq(1, 2, 3).toDS()
primitiveDS.map(_ + 1).collect()
val path = "people.json"
val peopleDS = spark.read.json(path).as[Person]
peopleDS.show()


Hive操作
在Spark 2.0中，是支持读写hive中存储的数据的。但是，因为hive有较多的依赖，所以默认情况下，这些依赖没有包含
在spark的发布包中。如果hive依赖可以在classpath路径中，那么spark会自动加载这些依赖。这些hive依赖必须在所有
的worker node上都放一份，因为worker node上运行的作业都需要使用hive依赖的序列化与反序列化包来访问hive中的
数据。
只要将hive-site.xml、 hdfs-site.xml和core-site.xml都放入spark/conf目录下即可。
如果要操作Hive，那么构建SparkSession的时候，就必须启用Hive支持，包括连接到hive的元数据库，支持使用hive序列
化与反序列化包，以及支持hive udf函数。如果我们没有安装hive，也是可以启用hive支持的。如果我们没有放置hivesite.xml到spark/conf目录下， SparkSession就会自动在当前目录创建元数据库，同时创建一个spark.sql.warehouse.dir参数
设置的目录，该参数的值默认是当前目录下的spark-warehouse目录。在spark 2.0中， hive.metastore.warehouse.dir属性
已经过时了，现在使用 spark.sql.warehouse.dir属性来指定hive元数据库的位置。


Hive操作
case class Record(key: Int, value: String)
val warehouseLocation = "file:${system:user.dir}/spark-warehouse"
val spark = SparkSession
.builder()
.appName("Spark Hive Example")
.config("spark.sql.warehouse.dir", warehouseLocation)
.enableHiveSupport()
.getOrCreate()
import spark.implicits._
import spark.sql


Hive操作
sql("CREATE TABLE IF NOT EXISTS src (key INT, value STRING)")
sql("LOAD DATA LOCAL INPATH 'kv1.txt' INTO TABLE src")
sql("SELECT * FROM src").show()
sql("SELECT COUNT(*) FROM src").show()
val sqlDF = sql("SELECT key, value FROM src WHERE key < 10 ORDER BY key")
val stringsDS = sqlDF.map {
case Row(key: Int, value: String) => s"Key: $key, Value: $value"
}
stringsDS.show()
val recordsDF = spark.createDataFrame((1 to 100).map(i => Record(i, s"val_$i")))
recordsDF.createOrReplaceTempView("records")
sql("SELECT * FROM records r JOIN src s ON r.key = s.key").show()


Hive 1.2.1安装
spark 2.0，默认是跟hive 1.2.1进行整合的，所以之前我们安装的是hive 0.13.1是不Ok的，实
际跑的时候会出现hive 0.13支持的一些操作， spark 2.0会用自己内置的hive 1.2.1 lib去操作和
访问我们的hive 0.13（包括metastore service），出现版本不一致的问题
1、将/usr/local/hive删除
2、将apache-hive-1.2.1-bin.tar.gz使用WinSCP上传到spark1的/usr/local目录下。
3、解压缩hive安装包： tar -zxvf apache-hive-1.2.1-bin.tar.gz。
4、重命名hive目录： mv apache-hive-1.2.1-bin hive
5、 cp /usr/share/java/mysql-connector-java-5.1.17.jar /usr/local/hive/lib


Hive 1.2.1安装
mv hive-default.xml.template hive-site.xml
vi hive-site.xml
<property>
<name>javax.jdo.option.ConnectionURL</name>
<value>jdbc:mysql://spark2upgrade01:3306/hive_metadata?createDatabaseIfNotExist=true</value>
</property>
<property>
<name>javax.jdo.option.ConnectionDriverName</name>
<value>com.mysql.jdbc.Driver</value>
</property>
<property>
<name>javax.jdo.option.ConnectionUserName</name>
<value>hive</value>
</property>
<property>
<name>javax.jdo.option.ConnectionPassword</name>
<value>hive</value>
</property>
<property>
<name>hive.metastore.uris</name>
<value>thrift://spark2upgrade01:9083</value>
</property>


Hive 1.2.1安装
把hive-site.xml中所有${system:java.io.tmpdir}全部替换为/usr/local/hive/iotmp
把hive-site.xml中所有${system:user.name}全部替换为root
rm -rf /usr/local/hadoop/share/hadoop/yarn/lib/jline-0.9.94.jar
cp /usr/local/hive/lib/jline-2.12.jar /usr/local/hadoop/share/hadoop/yarn/lib
mv hive-env.sh.template hive-env.sh
vi /usr/local/hive/bin/hive-config.sh
export JAVA_HOME=/usr/java/latest
export HIVE_HOME=/usr/local/hive
export HADOOP_HOME=/usr/local/hadoop


Hive 1.2.1安装
1、将hive-site.xml放置到spark的conf目录下
2、启动hive metastore service
hive --service metastore &


Hive 1.2.1安装
1、创建一份文件， students.txt，每行是一个学生的信息
2、 CREATE TABLE students(name string, age int, score double)
3、 LOAD DATA LOCAL INPATH '/usr/local/test_data/students.txt' INTO TABLE students
4、 spark-shell --master spark://spark2upgrade01:7077 --driver-memory 500m --executor-memory 500m
5、在spark-shell中，运行针对hive的sql语句
spark.sql(“select * from students”).show();
spark.sql(“select name from students where score>=90;”).show();
spark.sql(“select name from students where age<=15;”).show();
6、观察是否有正确的结果
7、在spark web ui中检查是否有运行的作业记录


Spark 2.0-Structured Streaming：深入浅出的介绍



流式计算的现状
大多数的流式计算引擎（比如storm、 spark streaming等）都仅仅关注流数据的计算方面：比如使用一个map函数对一个流中每条数据都进行转
换，或者是用reduce函数对一批数据进行聚合。但是，实际上在大部分的流式计算应用中，远远不只是需要一个流式计算引擎那么简单。相
反的，流式计算仅仅在流式应用中占据一个部分而已。因此现在出现了一个新的名词，叫做持续计算/应用， continuous application。比如以下
一些持续应用的例子：
1、更新需要以服务形式实时提供出去的数据：例如，我们可能需要更新一份数据，然后其他用户会通过web应用来实时查询这些数据。这种
情况下，一个技术难题就是实时计算应用如何与实时数据服务进行交互，比如说，当实时计算应用在更新数据的时候，如果用户通过实时数
据服务来进行查询，此时该如何处理？因此为了处理这种场景下的技术难题，就必须以一个完整的持续计算应用的方式来构建整个系统，而
不是站在实时计算的角度，仅仅考虑实时更新数据。
2、实时ETL（Extract、 Transform和Load） ：实时计算领域一个常见的应用就是，将一个存储系统中的数据转换后迁移至另外一个存储系统。
例如说，将JSON格式的日志数据迁移到Hive表中。这种场景下的技术难题就在于，如何与两边的存储系统进行交互，从而保证数据不会丢失，
同时也不会发生重复。这种协调逻辑是非常复杂的。
3、为一个已经存在的批量计算作业开发一个对应的实时计算作业：这个场景的技术难题在于，大多数的流式计算引擎都无法保证说，它们计
算出的结果是与离线计算结果相匹配的。例如说，有些企业会通过实时计算应用来构建实时更新的dashboard，然后通过批量计算应用来构建
每天的数据报表，此时很多用户就会发现并且抱怨，离线报表与实时dashboard的指标是不一致的。
4、在线机器学习：这类持续计算应用，通常都包含了大型的静态数据集以及批处理作业，还有实时数据流以及实时预测服务等各个组件。


流式计算的现状
以上这些例子就表明了在一个大型的流式计算应用中，流式计算本身其实只是占据
了一个部分而已，其他部分还包括了数据服务、存储以及批处理作业。但是目前的
现状是，几乎所有的流式计算引擎都仅仅是关注自己的那一小部分而已，仅仅是做
流式计算处理。这就使得开发人员需要去处理复杂的流式计算应用与外部存储系统
之间的交互，比如说管理事务，同时保证他们的流式计算结果与离线批处理计算结
果保持一致。这就是目前流式计算领域急需要解决的难题与现状。


持续计算应用
持续计算应用可以定义为，对数据进行实时处理的整套应用系统。 spark社区希望能够让开发人员仅仅使用一套api，就可以完整持续计算应用
中各个部分涉及的任务和操作，而这各个部分的任务和操作目前都是通过分离的单个系统来完成的，比如说实时数据查询服务，以及与批处
理作业的交互等。举例来说，未来对于解决这些问题的一些设想如下：
1、更新那些需要被实时提供服务的数据：开发人员可以开发一个spark应用，来同时完成更新实时数据，以及提供实时数据查询服务，可能是
通过jdbc相关接口来实现。也可以通过内置的api来实现事务性的、批量的数据更新，对一些诸如mysql、 redis等存储系统。
2、实时ETL：开发人员仅仅需要如同批处理作业一样，开发一样的数据转换操作，然后spark就可以自动完成针对存储系统的操作，并且保证
数据的一次且仅一次的强一致性语义。
3、为一个批处理作业开发一个实时版本： spark可以保证实时处理作业与批处理作业的结果一定是一致的。
4、在线机器学习：机器学习的api将会同时支持实时训练、定期批量训练、以及实时预测服务。
纯粹的实时计算应
用
输入流 流式计算 外部存储（事务）
跟其他应用进行交
互
持续计算应用
输入流 持续计算
静态数
据
实时查询
外部存储（事务）
一致性
静态数
据


Structured Streaming
Spark 2.0中，引入的structured streaming，就是为了实现上述所说的continuous application，也就是持续计算的。首先， structured streaming是
一种比spark更高阶的api，主要是基于spark的批处理中的高阶api，比如dataset/dataframe。此外， structured streaming也提供很多其他流式计
算应用所无法提供的功能：
1、保证与批处理作业的强一致性：开发人员可以通过dataset/dataframe api以开发批处理作业的方式来开发流式处理作业，进而structured
streaming可以以增量的方式来运行这些计算操作。在任何时刻，流式处理作业的计算结果，都与处理同一份batch数据的批处理作业的计算结
果，是完全一致的。而大多数的流式计算引擎，比如storm、 kafka stream、 flink等，是无法提供这种保证的。
2、与存储系统进行事务性的整合： structured streaming在设计时就考虑到了，要能够基于存储系统保证数据被处理一次且仅一次，同时能够
以事务的方式来操作存储系统，这样的话，对外提供服务的实时数据才能在任何时刻都保持一致性。目前spark 2.0版本的structured streaming，
仅仅支持hdfs这一种外部存储，在未来的版本中，会加入更多的外部存储的支持。事务性的更新是流式计算开发人员的一大痛点，其他的流式
计算引擎都需要我们手动来实现，而structured streaming希望在内核中自动来实现。
3、与spark的其他部分进行无缝整合： structured steaming在未来将支持基于spark sql和jdbc来对streaming state进行实时查询，同时提供与mllib
进行整合。 spark 2.0仅仅开始做这些整合的工作，在未来的版本中会逐渐完善这些整合。
除了这些独一无二的特性以外， structured streaming还会提供其他feature来简化流式应用的开发，例如对event time的支持，从而可以自动处
理延迟到达的数据，以及对滑动窗口和会话的更多的支持。目前structured streaming还停留在beta阶段，因此官方声明，仅供用户学习、实验
和测试。


Structured Streaming的未来
spark官方对structured streaming未来的计划是非常有野心的：希望spark的所有组件
（core、 sql、 dataset、 mllib等）都能够通过structured steaming，以增量的方式来运
行，进而支持更丰富的实时计算操作。 structured streaming会设计为让其计算结果
与批处理计算结果是强一致的。大数据用户的一个非常大的痛点，就是需要一个完
全统一的编程接口。例如说，之前用户进行大数据开发时，需要整合使用多种计算
引擎，比如mapreduce来进行etl， hive来执行sql查询， giraph来进行图计算， storm
来进行实时计算，等等。而spark则可以完全统一这些操作。此外， structured
streaming也希望能够完全涵盖一个持续计算应用中的方方面面。


Structured Streaming与其他流式计算应用的对比
属性 	Structure
d
Streaming	Spark
Streaming	Apache
Storm	Apache
Flink	Kafka
Stream	Google
Dataflow
Streaming
API	增量执行
批处理计
算	基于批处
理计算引
擎	与批处理
无关	与批处理
无关	与批处理
无关	基于批处
理计算引
擎
基于数据
位置前缀
的计算完
整性的保
证	√ 	√ 	× 	× 	× 	×
一致性语
义	exactly
once	exactly
once	exactly
once	exactly
once	at least
once	exactly
once
事务性操
作存储支
持	√ 	部分 	部分 	部分 	× 	×
交互式查
询	√ 	√ 	√ 	× 	× 	×
与静态数
据进行	√ 	√ 	× 	× 	× 	×




Spark 2.0-Structured Streaming：wordcount入门案例



Structured Streaming
structured streaming是一种可伸缩的、容错的、基于Spark SQL引擎的流式计算引擎。你可以使
用，与针对静态数据的批处理计算操作一样的方式来编写流式计算操作。随着数据不断地到达，
Spark SQL引擎会以一种增量的方式来执行这些操作，并且持续更新结算结果。可以使用java、
scala等编程语言，以及dataset/dataframe api来编写计算操作，执行数据流的聚合、基于event
的滑动窗口、流式数据与离线数据的join等操作。所有这些操作都与Spark SQL使用一套引擎来
执行。此外， structured streaming会通过checkpoint和预写日志等机制来实现一次且仅一次的语
义。简单来说，对于开发人员来说，根本不用去考虑是流式计算，还是批处理，只要使用同样
的方式来编写计算操作即可， structured streaming在底层会自动去实现快速、可伸缩、容错、
一次且仅一次语义。
spark 2.0仅仅是提供beta版本的structured streaming，所有的相关api都是实验性质的。


WordCount入门案例
val spark = SparkSession
.builder
.appName("StructuredNetworkWordCount")
.getOrCreate()
import spark.implicits._


WordCount入门案例
val lines = spark.readStream
.format("socket")
.option("host", "localhost")
.option("port", 9999)
.load()
val words = lines.as[String].flatMap(_.split(" "))
val wordCounts = words.groupBy("value").count()


WordCount入门案例
val query = wordCounts.writeStream
.outputMode("complete")
.format("console")
.start()
query.awaitTermination()


WordCount入门案例
yum install -y nc
nc -lk 9999
Spark 2.0-Structured Streaming：编程模型



Structured Streaming
基础编程模型
structured streaming的核心理念，就是将数据流抽象成一张表，而源源不断过来的
数据是持续地添加到这个表中的。这就产生了一种全新的流式计算模型，与离线计
算模型是很类似的。你可以使用与在一个静态表中执行离线查询相同的方式来编写
流式查询。 spark会采用一种增量执行的方式来对表中源源不断的数据进行查询。我
们可以将输入数据流想象成是一张input table。数据流中每条新到达的数据，都可
以想象成是一条添加到表中的新数据。画图讲解。


Structured Streaming
基础编程模型
针对输入数据执行的查询，会产生一张result table。每个trigger interval，比如说1秒
钟，添加到input table中的新数据行，都会被增量地执行我们定义的查询操作，产
生的结果会更新到结果表中。当结果表被更新的时候，我们可能会希望将结果表中
变化的行写入一个外部存储中。画图讲解。


Structured Streaming
基础编程模型
我们可以定义每次结果表中的数据更新时，以何种方式，将哪些数据写入外部存储。我们有多种模式的output：
• complete mode，被更新后的整个结果表中的数据，都会被写入外部存储。具体如何写入，是根据不同的外部存储自
身来决定的。
• append mode，只有最近一次trigger之后，新增加到result table中的数据，会被写入外部存储。只有当我们确定，
result table中已有的数据是肯定不会被改变时，才应该使用append mode。
• update mode，只有最近一次trigger之后， result table中被更新的数据，包括增加的和修改的，会被写入外部存储中。
spark 2.0中还不支持这种mode。这种mode和complete mode不同，没有改变的数据是不会写入外部存储的。
我们可以以上一讲的wordcount例子作为背景来理解， lines dataframe是一个input table，而wordcounts dataframe就是一
个result table。当应用启动后， spark会周期性地check socket输入源中是否有新数据到达。如果有新数据到达，那么
spark会将之前的计算结果与新到达的数据整合起来，以增量的方式来运行我们定义的计算操作，进而计算出最新的单
词计数结果。


Structured Streaming
基础编程模型
这种模型跟其他很多流式计算引擎都不同。大多数流式计算引擎都需要开发人员自
己来维护新数据与历史数据的整合并进行聚合操作。然后我们就需要自己去考虑和
实现容错机制、数据一致性的语义等。然而在structured streaming的这种模式下，
spark会负责将新到达的数据与历史数据进行整合，并完成正确的计算操作，同时更
新result table，不需要我们去考虑这些事情。画图讲解。


Structured Streaming
event-time和late-data process
event-time指的是嵌入在数据自身内部的一个时间。在很多流式计算应用中，我们可能都需要根据event-time
来进行处理。例如，可能我们需要获取某个设备每分钟产生的事件的数量，那么我们就需要使用事件产生时
的时间，而不是spark接受到这条数据的时间。设备产生的每个事件都是input table中的一行数据，而eventtime就是这行数据的一个字段。这就可以支持我们进行基于时间窗口的聚合操作（例如每分钟的事件数量），
只要针对input table中的event-time字段进行分组和聚合即可。每个时间窗口就是一个分组，而每一行都可以
落入不同行的分组内。因此，类似这样的基于时间窗口的分组聚合操作，既可以被定义在一份静态数据上，
也可以被定义在一个实时数据流上。
此外，这种模型也天然支持延迟到达的数据， late-data。 spark会负责更新result table，因此它有决定的控制权
来针对延迟到达的数据进行聚合结果的重新计算。虽然目前在spark 2.0中还没有实现这个feature，但是未来
会基于event-time watermark（水印）来实现这个late-data processing的feature。


Structured Streaming
容错语义
structured streaming的核心设计理念和目标之一，就是支持一次且仅一次的语义。为了实现这个目标，
structured streaming设计将source、 sink和execution engine来追踪计算处理的进度，这样就可以在任何一个步
骤出现失败时自动重试。每个streaming source都被设计成支持offset，进而可以让spark来追踪读取的位置。
spark基于checkpoint和wal来持久化保存每个trigger interval内处理的offset的范围。 sink被设计成可以支持在多
次计算处理时保持幂等性，就是说，用同样的一批数据，无论多少次去更新sink，都会保持一致和相同的状
态。这样的话，综合利用基于offset的source，基于checkpoint和wal的execution engine，以及基于幂等性的
sink，可以支持完整的一次且仅一次的语义。

Spark 2.0-Structured Streaming：
创建流式的dataset和dataframe



创建流式的dataset和dataframe
流式dataframe可以通过DataStreamReader接口来创建， DataStreamReader对象是通过
SparkSession的readStream()方法返回的。与创建静态dataframe的read()方法类似，我们可以指
定数据源的一些配置信息，比如data format、 schema、 option等。 spark 2.0中初步提供了一些
内置的source支持。
• file source：以数据流的方式读取一个目录中的文件。支持text、 csv、 json、 parquet等文件
类型。文件必须是被移动到目录中的，比如用mv命令。
• socket source：从socket连接中读取文本内容。 driver是负责监听请求的server socket。 socket
source只能被用来进行测试。


创建流式的dataset和dataframe
val socketDF = spark
.readStream
.format("socket")
.option("host", "localhost")
.option("port", 9999)
.load()
socketDF.isStreaming
socketDF.printSchema
val userSchema = new StructType().add("name", "string").add("age", "integer")
val csvDF = spark
.readStream
.option("sep", ";")
.schema(userSchema)
.csv("/path/to/directory")


创建流式的dataset和dataframe
上面的例子都是产生untyped类型的dataframe，这就意味着在编译时是无法检查其
schema的，只有在计算被提交并运行时才会进行检查。一些操作，比如map、
flatMap等，需要在编译时就知道具体的类型。为了使用一些typed类型的操作，我
们可以将dataframe转换为typed类型的dataset，比如df.as[String]。
Spark 2.0-Structured Streaming：对流式的dataset和dataframe执行计算操作



基础操作：选择、映射、聚合
我们可以对流式dataset/dataframe执行所有类型的操作，包括untyped操作， SQL类操作， typed操作。
case class DeviceData(device: String, type: String, signal: Double, time: DateTime)
val df: DataFrame = ... // streaming DataFrame with IOT device data with schema { device: string, type: string, signal: double, time: string }
val ds: Dataset[DeviceData] = df.as[DeviceData] // streaming Dataset with IOT device data
// Select the devices which have signal more than 10
df.select("device").where("signal > 10") // using untyped APIs
ds.filter(_.signal > 10).map(_.device) // using typed APIs
// Running count of the number of updates for each device type
df.groupBy("type").count() // using untyped API
// Running average signal for each device type
Import org.apache.spark.sql.expressions.scalalang.typed._
ds.groupByKey(_.type).agg(typed.avg(_.signal)) // using typed API


滑动窗口：基于event-time
import spark.implicits._
val words = ... // streaming DataFrame of schema { timestamp: Timestamp, word: String }
// Group the data by window and word and compute the count of each group
val windowedCounts = words.groupBy(
window($"timestamp", "10 minutes", "5 minutes"),
$"word"
).count()


滑动窗口：基于event-time
输入流
12:02 	hello
you
12:03 	hello
hello

12： 07 	me you

12:11 	me
12:13 	hello

时间轴
12:0
0
12:0
5
12:1
0
12:1
5
result table
12:00-
12:10	you 	1
12:00-
12:10	hello 	3

12:00-
12:10	you 	2
12:00-
12:10	dog 	3
12:00-
12:10	me 	1
12:05-
12:15	me 	1
12:05-
12:15	you 	1

12:00-
12:10	you 	2
12:00-
12:10	dog 	3
12:00-
12:10	me 	1
12:05-
12:15	me 	2
12:05-
12:15	you 	1
12:05-
12:15	hello 	1
12:10-
12:20	me 	1
12:10-
12:20	hello 	1




滑动窗口：基于event-time
输入流
12:02 	hello
you
12:03 	hello
hello

12： 07 	me you

12:11 	me
12:13 	hello
12:04 	hello

时间轴
12:0
0
12:0
5
12:1
0
12:1
5
result table
12:00-
12:10	you 	1
12:00-
12:10	hello 	3

12:00-
12:10	you 	2
12:00-
12:10	hello 	3
12:00-
12:10	me 	1
12:05-
12:15	me 	1
12:05-
12:15	you 	1

12:00-
12:10	you 	2
12:00-
12:10	hello 	4
12:00-
12:10	me 	1
12:05-
12:15	me 	2
12:05-
12:15	you 	1
12:05-
12:15	hello 	1
12:10-
12:20	me 	1
12:10-
12:20	hello 	1




join操作
structured streaming，支持将一个流式dataset与一个静态dataset进行join。
val staticDf = spark.read. ...
val streamingDf = spark.readStream. ...
streamingDf.join(staticDf, “type” ) // inner equi-join with a static DF
streamingDf.join(staticDf, “type” , “right_join” ) // right outer join with a static DF


不支持的操作
• streaming dataframe的chain aggregation
• limit and take
• distinct
• sort：仅在聚合过后，同时使用complete output mode时可用
• streaming dataframe和static dataframe的outer join
• full outer join是不支持的
• streaming dataframe在左侧时， left outer join是不支持的
• streaming dataframe在右侧时， right outer join是不支持的
• 两个streaming dataframe的join是不支持的
• count() -> groupBy().count()
• foreach() -> df.writeStream.foreach()
• show() -> console output sink

Spark 2.0-Structured Streaming：output mode、 sink以及
foreach sink详解



output操作
定义好了各种计算操作之后，就需要启动这个应用。此时就需要使用DataStreamWriter，通过spark.writeStream()方法返
回。此时需要指定以下一些信息：
• output sink的一些细节：数据格式、位置等。
• output mode：以哪种方式将result table的数据写入sink。
• query name：指定查询的标识。
• trigger interval：如果不指定，那么默认就会尽可能快速地处理数据，只要之前的数据处理完，就会立即处理下一
条数据。如果上一个数据还没处理完，而这一个trigger也错过了，那么会一起放入下一个trigger再处理。
• checkpoint地址：对于某些sink，可以做到一次且仅一次的语义，此时需要指定一个目录，进而可以将一些元信息
写入其中。一般会是类似hdfs上的容错目录。


output mode
目前仅仅支持两种output mode
• append mode：仅适用于不包含聚合操作的查询。
• complete mode：仅适用于包含聚合操作的查询。


output sink
目前有一些内置支持的sink
• file sink：在spark 2.0中，仅仅支持parquet文件，以及append模式
• foreach sink
• console sink：仅供调试
• memory sink：仅供调试


output sink
val noAggDF = deviceDataDf.select("device").where("signal > 10")
noAggDF
.writeStream
.format("console")
.start()
noAggDF
.writeStream
.parquet("path/to/destination/directory")
.start()
val aggDF = df.groupBy(“device”).count()
aggDF
.writeStream
.outputMode("complete")
.format("console")
.start()


output sink
aggDF
.writeStream
.queryName("aggregates") // this query name will be the table name
.outputMode("complete")
.format("memory")
.start()
spark.sql("select * from aggregates").show() // interactively query in-memory table


foreach sink详解
使用foreach sink时，我们需要自定义ForeachWriter，并且自定义处理每条数据的业务逻辑。每次trigger发生后，根据output mode需要写入sink
的数据，就会传递给ForeachWriter来进行处理。使用如下方式来定义ForeachWriter：
datasetOfString.write.foreach(new ForeachWriter[String] {
def open(partitionId: Long, version: Long): Boolean = {
// open connection
}
def process(record: String) = {
// write string to connection
}
def close(errorOrNull: Throwable): Unit = {
// close the connection
}
})


foreach sink详解
需要有如下一些注意点：
• ForeachWriter必须支持序列化，因为该对象会被序列化后发送到executor上去执行。
• open、 process和close这三个方法都会给executor调用。
• ForeachWriter所有的初始化方法，必须创建数据库连接，开启一个事务，打开一个IO流等等，都必须在open方法中
完成。必须注意，如果在ForeachWriter的构造函数中进行初始化，那么这些操作都是在driver上发生的。
• open中有两个参数， version和partition，可以唯一标识一批需要处理的数据。每次发生一次trigger， version就会自增
长一次。 partition是要处理的结果数据的分区号。因为output操作是分布式执行的，会分布在多个executor上并行执
行。
• open可以使用version和partition来决定，是否要处理这一批数据。此时可以选择返回true或false。如果返回false，那
么process不会被调用。举个例子来说，有些partition的数据可能已经被持久化了，而另外一些partiton的处理操作由
于失败被重试，此时之前已经被持久化的数据可以不再次进行持久化，避免重复计算。
• close方法中，需要处理一些异常，以及一些资源的释放。
Spark 2.0-Structured Streaming：管理streaming query



管理streaming query
val query = df.writeStream.format("console").start() // get the query object
query.id // get the unique identifier of the running query
query.name // get the name of the auto-generated or user-specified name
query.explain() // print detailed explanations of the query
query.stop() // stop the query
query.awaitTermination() // block until query is terminated, with stop() or with error
query.exception() // the exception if the query has been terminated with error
query.sourceStatus() // progress information about data has been read from the input sources
query.sinkStatus() // progress information about data written to the output sink


管理streaming query
val spark: SparkSession = ...
spark.streams.active // get the list of currently active streaming queries
spark.streams.get(id) // get a query object by its unique id
spark.streams.awaitAnyTermination() // block until any one of them terminates

Spark 2.0-Structured Streaming：基于checkpoint的容错机
制



容错机制
如果实时计算作业遇到了某个错误挂掉了，那么我们可以配置容错机制让它自动重启，同时继续之前的进度运行下去。
这是通过checkpoint和wal机制完成的。可以给query配置一个checkpoint location，接着query会将所有的元信息（比如
每个trigger消费的offset范围、至今为止的聚合结果数据），写入checkpoint目录。
aggDF
.writeStream
.outputMode("complete")
.option(“checkpointLocation” , “path/to/HDFS/dir” )
.format("memory")
.start()


Spark面试、简历中的项目编写以及实际生产环境的集群和资源配置等



学完课程以后找什么样的工作
误区：学完课程，不是找所谓的spark工程师的工作。第一方面，实际上，企业单纯只招聘spark开发人员，目前这种需
求还是比较少的；第二方面，你真正开始做spark相关开发工作，你也不可能只是做spark开发呀，可能会涉及到hdfs、
hbase、 redis、 memcached、 mysql等等。
大数据开发工程师。
hadoop，只会hadoop， 3~5年，找大数据工作，可能就被淘汰了。
行业发展的趋势是：大数据的项目和应用，都是综合运行各种技术组合而成，包括hadoop、 spark、 storm、 zookeeper、
机器学习等等。你还只会hadoop，企业要的是同时会hadoop、 spark和storm、数据挖掘等等，你就被淘汰了。
学spark，不是说让你找一份马上就是高薪的spark的工作。而是说，这是一个大趋势，大潮流，如果你现在不学， 2年
以后再学，就晚了，你就比别人慢了2年，慢了两拍子。学spark，是为了以后不被淘汰，以及跟上行业和技术发展的
大潮流。
不是说学了spark，工资马上就翻倍。。。。。


企业对大数据工程师的一些招聘要求
中小型公司（或者有些虽然是大公司，但是大数据上做的比较low）
多面手，大数据相关的活儿，基本都能搞定
hadoop（hdfs、 yarn、 mapreduce、 hive、 zookeeper、 hbase、 flume、 kafka），掌握基础就够去小公司了
storm，更好了，加分项， +，说明你还懂实时计算，基础
spark，越来越多的公司意识大spark在高性能大数据计算上的应用，以及统一大数据计算栈的价值（离线+流式+机器学
习+SQL），大大的加分项，因为spark是未来的主流和潮流，熟练->精通的程度
java+j2ee的开发：简单能掌握到常用开源框架的整合和使用，即可
懂一点数据挖掘，懂一些机器学习的算法
懂一点集群运维，能够搞一搞小型集群的运维和管理


企业对大数据工程师的一些招聘要求
大公司（BAT、京东、小米等，前10以内的互联网大公司）
数据开发： hadoop（mr+hive+hbase使用和开发）、 storm开发、 spark开发、数据仓库的建模、 j2ee的开发
• 大公司的要求更加高，比如说hadoop、 storm、 spark，可能就不仅仅只是基础了，要求熟练，和原理，甚至是源码，还有较为丰富的使用
经验
• 数据仓库建模：行业里，有经验，做的好的，很少
• j2ee：精通java，能够进行大型j2ee大数据平台的架构
集群运维：高达数百，甚至数千个几点的大型集群的运维和管理
• 精通某一个，或某个大数据引擎的源码（比如hdfs、 spark）
• 精通一种，或几种编程语言（比如c++， java）
• 精通计算机基础知识，比如磁盘，网络， IO，数据库， CPU，操作系统
• 能够对大数据引擎进行源码级的二次开发
数据挖掘：基于大数据的一些技术，比如spark mllib， hadoop mahout，做一些数据挖掘，推荐，排序，搜索等应用
• 要求， 211硕士以上出身
• 精通各种算法，包括机器学习、深度学习等
• 能够使用大数据生态， hadoop、 spark和storm
• 掌握一到两种编程语言： python、 java


如何搞定hadoop、 storm、j2ee等其他技术，以丰富自己的经验
首先，看到这里的话，把咱们这套课程，好好看上2~3遍，技术、代码搞的很熟，甚至跟着课程精读一下源码。那至
少你可以达到spark熟练，甚至精通的程度。就绝对不只是基础的一个掌握。
如果你对自己的定位，就是拿个20k左右，或者15k~20k。那么其实，基本上，你只要补一下hadoop的基础，有时间和
兴趣掌握一点storm的基础，再加上自己如果本身有一些java的基础，再补一补j2ee的基础的东西。也就再多花个2~3个
月，最多，其实你就可以达到我上面说的去面试小公司的程度了。
机器学习、数据挖掘，不知道也罢。企业也不是傻子。
hadoop：网上搜一些视频， 10天~15天，看看，基本上基础就有了，就够了
storm：同上， 10~15天
j2ee：同上， 1~2个月


如何写简历，如何写项目以及spark项目
无论你现在是干嘛的，无论工作几年，反正你就把至少最近3~5年，都写成大数据相关的经历。
简历上描述自己的技术，两个要点，第一，突出自己的技术栈的丰富程度，比如hadoop、 storm、 spark、 j2ee；第二，加强和突出你自己最擅
长的某一两个技术，比如说spark。写上精读spark 1.3版本的源码深入理解***机制，比如说资源调度算法、 stage划分算法；熟悉各种常见spark
性能优化技术等。
简历上写项目：一定要至少写2~3个大数据相关的项目，最好是hadoop、 storm、 spark，还有j2ee相关的项目，都有。项目从哪儿来，其实就
是从你学习的课程里面来，首先无论你是学什么视频课程，比如说我们这里的spark课程，或者是其他你在网上找到的课程，都会带上少数，
或者大量的案例。你得把这些案例，跟你自己目前在公司里面做的项目，或者业务，结合起来。
前提：绝对不要照搬照抄所有视频课程里学到的案例，互联网广告的黑名单过滤，这个太不靠谱！！！
把学到的案例，和你公司的业务，深度结合起来。比如说，你现在的公司是电信、银行，或者是网络运维，那么你就可以写：电信行业计费
日志实时统计系统，银行信用卡消费日志实时统计系统，网络运维日志实时统计系统；电信黑名单用户过滤；银行信用卡申办实时黑名单过
滤；网络运维黑名单实时过滤。
你应该尝试主动在公司里自己去做这个结合以后想出来的项目，跟领导去提一提，或者你就自己做，接到数据，用跟公司一样的数据格式，
自己造一些数据，然后自己用这些造的数据，模仿课程想出来的业务和功能，自己实际去写一写，跑一跑。


用哪些渠道来投递简历和找工作
1、拉勾网：互联网， IT领域，岗位最多的，质量较好的
2、 boss直聘：企业直接对接候选人
3、 100 offer：程序员拍卖
4、猎聘网：简历挂上去，猎头，等着猎头找你，也行
5、智联招聘、前程无忧
大数据、 hadoop、 spark
海投，尽可能多地投，不要挑公司，多投小公司，多争取面试机会


面试的一些要点
1、之前说的那些东西，视频课程里的东西，都掌握地非常扎实以后，其实你面试基本不会有太大的问题，技术这块
2、项目这块，自己在家里多练练，自己要琢磨出，自己现在的公司里，争取做出自己最熟悉的业务，属于自己的项
目：项目的架构、业务、数据量、数据格式、需求、功能、技术难题
3、刚开始面试，肯定会碰到很多的问题，比如自己答不出来的： spark streaming如何在挂掉重启以后，继续消费之前
的数据；项目里， 100g的数据量用多少资源处理； spark源码某个算法，自己忘了； java并发； java虚拟机。这场挂掉了
以后，回家赶紧上网查资料，找老师，或者找其他人，把今天问的不好的问题，全都给补了。
4、原则，这一次没答好，没答出来的问题，下一次必须答好了
5、面试的时候，注意一个技巧：人家问到你一个点，比如说集群规模，你此时必须带出课程相关的大量的知识，比
如集群运维和管理； spark streaming，问题， checkpoint、 WAL， --supervise， driver挂了自动重启； spark sql，调优，默
认shuffle并行度是200，我们后来调优变成了500，性能提升一倍


spark项目的集群规模以及处理数据量和资源
想做spark，但是没有人会做，所以想招一些懂的人过来做
小公司： 15~30台机器，配置，大概每台机器都是几十G内存，十几个cpu，就ok了，具体什么型号的服务器，自己上
京东搜一搜
• 20 * 30G = 600G
• 20 * 12 core = 240 cpu core
数据量： 20G~30G
资源：一个spark作业，处理10G~20G的数据，大概需要100个executor， 3~5G内存， 3~5个cpu， 300~500G内存，
300~500个cpu
多长时间能处理完： 10分钟~30分钟


如何谈薪资
大部分学员，别指望一学好，出去工资瞬间就25k， 30k， 40k
学好spark，不是为了拿高工资，瞬间；你现在是做hadoop的，你要学spark，赶上技术潮流，不被淘汰；你现在不是搞大数据的，那只能你能
顺利转型，先进去这个行业，然后慢慢熬，几年，能够比你原来的行业发展更好，就可以了
薪资，涨还是要涨的
1~3年，如果已经是做hadoop的人，大数据行业里的人，十几k，试着往20k去要
3~5年，之前都是做hadoop的人，现在你本身应该就差不多20k， 18k~20k，试着往25k~30去要
如果你本身就是20~30k之间的，那么你应该寻求一个很好的职位，比如说架构师，团队经理，尝试往30++要
1-3年， 12k~15k，差不多，先转行再说
3~5年， 15~20k，你首先保证20k吧，再试着往25去要把
20~30k，你能保证跟目前薪资持平，能先转型就不错了，如果有特别欣赏你的人，要30+也可以试一试
大数据行业的薪资的现状和瓶颈： 30k
再想往上要，那基本要看运气，还有就是你自己的技术要修炼的过硬，要真的有较为丰富的经验，有深入的源码研究的功底，有广阔的技术
面，然后去争取一个大数据架构师的职位，或者是大数据技术经理的职位


重复、重复、再重复
1、自己的定位是什么？
2、自己跟公司的招聘需求差了什么？
3、自己如何通过找一些课程来弥补自己的技术？
4、如何把简历写的更好，更有吸引力？
5、拓宽一些渠道，更多，更广泛地投递简历
6、如何把面试面的更好，每次面试都能hold住？
7、认清现实，大概清楚自己目前应该拿的薪资范围，要合理的薪资
