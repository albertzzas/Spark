ENV List

		
VM	VituralBox	
	网络	桥接bridge
远程控制	SecureCRT	
文件传输	WinSCP	VMare时用自带的本机分享最快，samba，ftp啥的费事
Linux	CentOS-6.5-i386-minimal.iso	
	User	root权限使用，以后可以扩展用户，像oracle那样配置
	ssh	
	PATH	~/.bahcrc
Java	1.7	
MySQL		yum 安装
Hadoop	2.4	
Hive	0.13	
Zookeeper		
Kafka		
Spark	2.1.1	

PATH文件

vi ~/.bashrc
source ~/.bashrc

export JAVA_HOME=/usr/java/latest
export PATH=$PATH:$JAVA_HOME/bin:PATH


安装顺序
1网络配置，先给个IP ，然后用远程复制粘贴
2配置静态固定IP，几个节点互通，可连接外网
3安装基础命令与软件，yum,ssh
4单个机器安装，都按ssh, 其他软件推到其他机器上，注意环境变量，单个软件的测试
5有依赖关系的按顺序装，如最后按spark
6少量文件改用文件传来传去， 在WIN上改多爽， 但是批量的考虑批处理吧，后者三剑客啥的。。。

常用命令：

帮助：
 
help 是内部命令的帮助,比如cd命令
man 是外部命令的帮助，比如ls命令

OS
ll –a
mkdir 
cp
mv
chmod


压缩解压：

tar -zxvf hadoop-2.4.1.tar.gz
rpm -ivh jdk-7u65-linux-i586.rpm
unzip xxx.zip
打印管道类（cat显示有上现）:
More/ less   正着  倒着
Head
Tail   tail -3 test.txt  ,跟踪尾部
Cat  多行打印  看末尾 /   Tac  倒着显示
Echo  单行打印
查询类
Grep
在说linux正规表达式之前，还介绍下linux中查找文本文件常用的三个命令：
1.grep : 最早的文本匹配程序，使用POSIX定义的基本正则表达式（BRE）来匹配文本。
2.egrep : 扩展式grep，其使用扩展式正规表达式（ERE）来匹配文本。
3.fgrep ： 快速grep，这个版本匹配固定字符串而非正则表达式。并且是唯一可以并行匹配多个字符串的版本。

Find
sed
awk

PATH相关：
 export

网络
vi /etc/sysconfig/network-scripts/ifcfg-eth0

vi /etc/hosts
service iptables stop
service ip6tables stop
chkconfig iptables off
chkconfig ip6tables off

vi /etc/selinux/config

rpm -ivh jdk-7u65-linux-i586.rpm


安装yum
yum clean all
yum makecache
yum install telnet

安装mysql

yum install -y mysql-server
service mysqld start
chkconfig mysqld on

3、使用yum安装mysql connector
yum install -y mysql-connector-java
4、将mysql connector拷贝到hive的lib包中
cp /usr/share/java/mysql-connector-java-5.1.17.jar /usr/local/hive/lib

ssh
ssh-keygen -t rsa
cd /root/.ssh
cp id_rsa.pub authorized_keys
ssh-copy-id -i sparkXXXXX

Hadoop

1、格式化namenode：在spark2upgrade01上执行以下命令hdfs namenode -format
2、启动hdfs集群： start-dfs.sh
3、验证启动是否成功： jps、 50070端口
spark2upgrade03： datanode

启动yarn集群
1、启动yarn集群： start-yarn.sh
2、验证启动是否成功： jps、 8088端口

在mysql上创建hive元数据库，并对hive进行授权
create database if not exists hive_metadata;
grant all privileges on hive_metadata.* to 'hive'@'%' identified by 'hive';
grant all privileges on hive_metadata.* to 'hive'@'localhost' identified by 'hive';
grant all privileges on hive_metadata.* to 'hive'@'spark2upgrade01' identified by 'hive';
flush privileges;
use hive_metadata;


验证：
B/S:
IP:50070端口
IP:8088端口
IP:8080端口

Shell:
Hadoop
1、格式化namenode：在spark2upgrade01上执行以下命令hdfs namenode -format
2、启动hdfs集群： start-dfs.sh
3、验证启动是否成功： jps、 50070端口
spark2upgrade01： namenode、 datanode、 secondarynamenode
spark2upgrade02： datanode
spark2upgrade03： datanode

启动yarn集群
1、启动yarn集群： start-yarn.sh
2、验证启动是否成功： jps、 8088端口
spark2upgrade01： resourcemanager、 nodemanager
spark2upgrade02： nodemanager
spark2upgrade03： nodemanager

启动spark集群
1、在spark目录下的sbin目录
2、执行./start-all.sh
3、使用jsp和8080端口可以检查集群是否启动成功
4、进入spark-shell查看是否正常

具体步骤
VM里要改配置 桥接， OS里terminal根据自己喜欢改，SecureCRT这里也可以改改个性化，主要要改删除符号啥的

本机 ipconfig –all,获得本地IP,DNS,GATEWAY,注意桥接的区别， 静态配置IP 
本地PC …/driver/host 映射写进去 几个节点的都写

Ifconfig eth0 XXXX给一个临时
配置网络
vi /etc/sysconfig/network-scripts/ifcfg-eth0
DEVICE=eth0
TYPE=Ethernet
ONBOOT=yes
BOOTPROTO=dhcp

service network restart
ifconfig

BOOTPROTO=static
IPADDR=192.168.0.X
NETMASK=255.255.255.0
GATEWAY=192.168.0.1

service network restart

vi /etc/hosts
配置本机的hostname到ip地址的映射

关闭防火墙
service iptables stop
service ip6tables stop
chkconfig iptables off
chkconfig ip6tables off
vi /etc/selinux/config
SELINUX=disabled
关闭windows的防火墙

安装yum
yum clean all
yum makecache
yum install telnet

安装JDK 1.7
1、将jdk-7u60-linux-i586.rpm通过WinSCP上传到虚拟机中
2、安装JDK： rpm -ivh jdk-7u65-linux-i586.rpm
3、配置jdk相关的环境变量
vi ~/.bashrc
export JAVA_HOME=/usr/java/latest
export PATH=$PATH:$JAVA_HOME/bin
source ~/.bashrc
4、测试jdk安装是否成功： java –version

两位两台一样都不能少

ssh

1、首先在三台机器上配置对本机的ssh免密码登录
ssh-keygen -t rsa
生成本机的公钥，过程中不断敲回车即可， ssh-keygen命令默认会将公钥放在/root/.ssh目录下
cd /root/.ssh
cp id_rsa.pub authorized_keys
将公钥复制为authorized_keys文件，此时使用ssh连接本机就不需要输入密码了
2、接着配置三台机器互相之间的ssh免密码登录
使用ssh-copy-id -i spark命令将本机的公钥拷贝到指定机器的authorized_keys文件中

Hadoop 2.4集群搭建

部署hadoop安装包
1、hadoop-2.4.1.tar.gz，使用WinSCP上传到CentOS的/usr/local目录下。
2、将hadoop包进行解压缩： tar -zxvf hadoop-2.4.1.tar.gz
3、对hadoop目录进行重命名： mv hadoop-2.4.1 hadoop
4、配置hadoop相关环境变量
vi ~/.bashrc
export HADOOP_HOME=/usr/local/hadoop
export PATH=$HADOOP_HOME/bin:$HADOOP_HOME/sbin
source ~/.bashrc
5、在/usr/local目录下创建data目录

修改core-site.xml配置文件
<property>
<name>fs.default.name</name>
<value>hdfs://spark2upgrade01:9000</value>
</property>
EDUCATION TO CREATE A BRIGHT FUTURE
Hadoop 2.4集群搭建
修改hdfs-site.xml配置文件
<property>
<name>dfs.name.dir</name>
<value>/usr/local/data/namenode</value>
</property>
<property>
<name>dfs.data.dir</name>
<value>/usr/local/data/datanode</value>
</property>
<property>
<name>dfs.tmp.dir</name>
<value>/usr/local/data/tmp</value>
</property>
<property>
<name>dfs.replication</name>
<value>3</value>
</property>

修改mapred-site.xml配置文件
<property>
<name>mapreduce.framework.name</name>
<value>yarn</value>
</property>


修改yarn-site.xml配置文件
<property>
<name>yarn.resourcemanager.hostname</name>
<value>spark2upgrade01</value>
</property>
<property>
<name>yarn.nodemanager.aux-services</name>
<value>mapreduce_shuffle</value>
</property>


修改slaves配置文(MV重命名)

spark2upgrade01
spark2upgrade02
spark2upgrade03

在另外两台机器上部署hadoop

1、使用scp命令将spark2upgrade01上面的hadoop安装包和~/.bashrc配置文件都拷贝过去。
2、要记得对.bashrc文件进行source，以让它生效。
3、记得在另外两台机器的/usr/local目录下创建data目录。

启动hdfs集群
1、格式化namenode：在spark2upgrade01上执行以下命令hdfs namenode -format
2、启动hdfs集群： start-dfs.sh
3、验证启动是否成功： jps、 50070端口
spark2upgrade01： namenode、 datanode、 secondarynamenode
spark2upgrade02： datanode
spark2upgrade03： datanode

启动yarn集群
1、启动yarn集群： start-yarn.sh
2、验证启动是否成功： jps、 8088端口
spark2upgrade01： resourcemanager、 nodemanager
spark2upgrade02： nodemanager
spark2upgrade03： nodemanager



Hive 0.13搭建
部署hive安装包
1、将课程提供的apache-hive-0.13.1-bin.tar.gz使用WinSCP上传到spark1的/usr/local目录下。
2、解压缩hive安装包： tar -zxvf apache-hive-0.13.1-bin.tar.gz。
3、重命名hive目录： mv apache-hive-0.13.1-bin hive
4、配置hive相关的环境变量
vi ~/.bashrc
export HIVE_HOME=/usr/local/hive
export PATH=$HIVE_HOME/bin
source ~/.bashrc

安装mysql

1、在spark01上安装mysql。
2、使用yum安装mysql server。
yum install -y mysql-server
service mysqld start
chkconfig mysqld on
3、使用yum安装mysql connector
yum install -y mysql-connector-java
4、将mysql connector拷贝到hive的lib包中
cp /usr/share/java/mysql-connector-java-5.1.17.jar /usr/local/hive/lib
5、在mysql上创建hive元数据库，并对hive进行授权
create database if not exists hive_metadata;
grant all privileges on hive_metadata.* to 'hive'@'%' identified by 'hive';
grant all privileges on hive_metadata.* to 'hive'@'localhost' identified by 'hive';
grant all privileges on hive_metadata.* to 'hive'@'spark2upgrade01' identified by 'hive';
flush privileges;
use hive_metadata;

修改hive-site.xml配置文件
mv hive-default.xml.template hive-site.xml

vi hive-site.xml

<property>
<name>javax.jdo.option.ConnectionURL</name>
<value>jdbc:mysql://spark2upgrade01:3306/hive_metadata?createDatabaseIfNotExist=true</value>
</property>
<property>
<name>javax.jdo.option.ConnectionDriverName</name>
<value>com.mysql.jdbc.Driver</value>
</property>
<property>
<name>javax.jdo.option.ConnectionUserName</name>
<value>hive</value>
</property>
<property>
<name>javax.jdo.option.ConnectionPassword</name>
<value>hive</value>
</property>

源文件里面有个少</property>2700行左右 ，手动加，注意报错内容

配置hive-env.sh和hive-config.sh
mv hive-env.sh.template hive-env.sh
vi /usr/local/hive/bin/hive-config.sh
export JAVA_HOME=/usr/java/latest
export HIVE_HOME=/usr/local/hive
export HADOOP_HOME=/usr/local/hadoop

验证安装是否成功
直接输入hive命令，可以进入hive命令行

安装scala 2.11(用Spark才有必要安， 要是一开始就知道要用， 可以早安 ，安装容易)
1、将课程提供的scala-2.11.4.tgz使用WinSCP拷贝到spark1的/usr/local目录下。
2、对scala-2.11.4.tgz进行解压缩： tar -zxvf scala-2.11.4.tgz。
3、对scala目录进行重命名： mv scala-2.11.4 scala
4、配置scala相关的环境变量
vi ~/.bashrc
export SCALA_HOME=/usr/local/scala
export PATH=$SCALA_HOME/bin
source ~/.bashrc
5、查看scala是否安装成功： scala -version
6、在另外两台机器上都安装scala，使用scp将scala和.bashrc拷贝到过去即可。


Spark 2.0集群搭建
部署spark 2.0安装包
1、将spark-2.0.0-bin-hadoop2.4.tgz使用WinSCP上传到/usr/local目录下。自己在
http://spark.apache.org/downloads.html上下载即可。
2、解压缩spark包： tar zxvf spark-2.0.0-bin-hadoop2.4.tgz。
3、更改spark目录名： mv spark-2.0.0-bin-hadoop2.4.tgz spark
4、设置spark环境变量
vi .bashrc
export SPARK_HOME=/usr/local/spark
export PATH=$SPARK_HOME/bin
export CLASSPATH=.:$CLASSPATH:$JAVA_HOME/lib:$JAVA_HOME/jre/lib
source .bashrc

配置spark-env.sh
1、 cd /usr/local/spark/conf
2、 cp spark-env.sh.template spark-env.sh
3、 vi spark-env.sh
export JAVA_HOME=/usr/java/latest
export SCALA_HOME=/usr/local/scala
export SPARK_MASTER_HOST=spark2upgrade01
export SPARK_WORKER_MEMORY=500m
export HADOOP_CONF_DIR=/usr/local/hadoop/etc/hadoop


配置slaves（MV）
spark2upgrade01
spark2upgrade02
spark2upgrade03
EDUCATION TO CREATE A BRIGHT FUTURE
Spark 2.0集群搭建
在另外两台机器上部署spark 2.0
在另外两个节点进行一模一样的配置，使用scp将spark和.bashrc拷贝到过去即可。


配置spark可以使用hive
1、将hive-site.xml放置到spark的conf目录下
2、修改spark/conf和hive/conf下的hive-site.xml
<property>
<name>hive.metastore.uris</name>
<value>thrift://spark2upgrade01:9083</value>
</property>

启动Hive会报错，替换提示字符

3、启动hive metastore service
hive --service metastore &
4、 cp hive/lib/mysql-connector-java-5.1.17.jar spark/jars/
5、 hdfs dfs -chmod 777 /tmp/hive-root (先自己建立folder)

启动spark集群
1、在spark目录下的sbin目录
2、执行./start-all.sh
3、使用jsp和8080端口可以检查集群是否启动成功
4、进入spark-shell查看是否正常

检查spark集群能否与hdfs整合使用
1、使用 spark-shell --master spark://spark2upgrade01:7077 --driver-memory 500m --executor-memory 500m启动
2、手工创建一份文件，上传到hdfs上去，放在/test_data/wordcount.txt中
3、在spark-shell中，基于hdfs上的文件，编写与运行一个wordcount程序
val lines = sc.textFile(“hdfs://spark2upgrade01:9000/test_data/wordcount.txt”)
val words = lines.flatMap(_.split(“ ”))
val pairs = words.map((_, 1))
val wordCounts = pairs.reduceByKey(_ + _)
wordCounts.collect().foreach(println(_))
4、观察是否有正确的结果
5、在spark web ui中检查是否有运行的作业记录

检查spark集群能否与hive整合使用
1、 创建一份文件， students.txt，每行是一个学生的信息
2、 CREATE TABLE students(name string, age int, score double)
3、 LOAD DATA LOCAL INPATH '/usr/local/test_data/students.txt' INTO TABLE students
4、 spark-shell --master spark://spark2upgrade01:7077 --driver-memory 500m --executor-memory 500m
5、在spark-shell中，运行针对hive的sql语句
spark.sql(“select * from students”).show();
spark.sql(“select name from students where score>=90”).show();
spark.sql(“select name from students where age<=15”).show();
6、观察是否有正确的结果
7、在spark web ui中检查是否有运行的作业记录

平时启动

基础配置

DB 服务Mysql服务
service mysqld start
chkconfig mysqld on
Hadoop服务
启动hdfs集群
1、格式化namenode：在spark2upgrade01上执行以下命令hdfs namenode -format
2、启动hdfs集群： start-dfs.sh
3、验证启动是否成功： jps、 50070端口
spark2upgrade01： namenode、 datanode、 secondarynamenode
spark2upgrade02： datanode
spark2upgrade03： datanode

启动yarn集群
1、启动yarn集群： start-yarn.sh
2、验证启动是否成功： jps、 8088端口
spark2upgrade01： resourcemanager、 nodemanager
spark2upgrade02： nodemanager
spark2upgrade03： nodemanager


启动spark集群
1、在spark目录下的sbin目录
2、执行./start-all.sh
3、使用jsp和8080端口可以检查集群是否启动成功
4、进入spark-shell查看是否正常

spark
spark-shell --master spark://spark2upgrade01:7077 --driver-memory 500m --executor-memory 500m
//加限制条件






